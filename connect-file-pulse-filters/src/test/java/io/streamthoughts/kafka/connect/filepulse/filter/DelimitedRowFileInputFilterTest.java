/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License. You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package io.streamthoughts.kafka.connect.filepulse.filter;

import io.streamthoughts.kafka.connect.filepulse.config.DelimitedRowFilterConfig;
import io.streamthoughts.kafka.connect.filepulse.offset.SourceMetadata;
import io.streamthoughts.kafka.connect.filepulse.reader.RecordsIterable;
import io.streamthoughts.kafka.connect.filepulse.source.FileInputData;
import io.streamthoughts.kafka.connect.filepulse.source.FileInputOffset;
import org.apache.kafka.connect.data.Schema;
import org.apache.kafka.connect.data.SchemaBuilder;
import org.apache.kafka.connect.data.Struct;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;

import java.io.File;
import java.io.IOException;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;

import static io.streamthoughts.kafka.connect.filepulse.config.DelimitedRowFilterConfig.READER_EXTRACT_COLUMN_NAME_CONFIG;
import static io.streamthoughts.kafka.connect.filepulse.config.DelimitedRowFilterConfig.READER_FIELD_COLUMNS_CONFIG;


public class DelimitedRowFileInputFilterTest {

    private static final String SEPARATOR_DEFAULT = DelimitedRowFilterConfig.READER_FIELD_SEPARATOR_DEFAULT;

    private Map<String, String> configs;

    private DelimitedRowFilter filter;

    private static final Schema DEFAULT_SCHEMA = SchemaBuilder.struct()
                                        .field("message", SchemaBuilder.array(SchemaBuilder.string()))
                                        .field("headers", SchemaBuilder.array(SchemaBuilder.string()))
                                        .field("footers", SchemaBuilder.array(SchemaBuilder.string()))
                                        .build();

    private static final Struct DEFAULT_STRUCT = new Struct(DEFAULT_SCHEMA)
                                                    .put("message", Collections.singletonList("value1;value2;value3"))
                                                    .put("headers", Collections.singletonList("col1;col2;col3"));

    /**
    @Rule
    public TemporaryFolder testFolder = new TemporaryFolder();
    **/

    private File file;

    private FilterContext context;

    @Before
    public void setUp() throws IOException {
        filter = new DelimitedRowFilter();
        configs = new HashMap<>();

        SourceMetadata metadata = new SourceMetadata("", "", 0L, 0L, 0L, -1L);
        context = InternalFilterContext.with(metadata, FileInputOffset.empty());
    }

    @Test
    public void shouldAutoGeneratedSchemaGivenNoSchemaField() {
        filter.configure(configs);
        RecordsIterable<FileInputData> output = filter.apply(null, new FileInputData(DEFAULT_STRUCT), false);
        Assert.assertNotNull(output);
        Assert.assertEquals(1, output.size());

        final FileInputData record = output.iterator().next();
        Assert.assertEquals("value1", record.getFirstValueForField("column1"));
        Assert.assertEquals("value2", record.getFirstValueForField("column2"));
        Assert.assertEquals("value3", record.getFirstValueForField("column3"));
    }

    @Test
    public void shouldExtractColumnNamesFromGivenField() {
        configs.put(READER_EXTRACT_COLUMN_NAME_CONFIG, "headers");
        filter.configure(configs);
        RecordsIterable<FileInputData> output = filter.apply(null, new FileInputData(DEFAULT_STRUCT), false);
        Assert.assertNotNull(output);
        Assert.assertEquals(1, output.size());

        final FileInputData record = output.iterator().next();
        Assert.assertEquals("value1", record.getFirstValueForField("col1"));
        Assert.assertEquals("value2", record.getFirstValueForField("col2"));
        Assert.assertEquals("value3", record.getFirstValueForField("col3"));
    }

    @Test
    public void shouldUseConfiguredSchemaWithNoType() {
        configs.put(READER_FIELD_COLUMNS_CONFIG, "c1:STRING;c2:STRING;c3:STRING");
        filter.configure(configs);
        RecordsIterable<FileInputData> output = filter.apply(null, new FileInputData(DEFAULT_STRUCT), false);
        Assert.assertNotNull(output);
        Assert.assertEquals(1, output.size());

        final FileInputData record = output.iterator().next();
        Assert.assertEquals("value1", record.getFirstValueForField("c1"));
        Assert.assertEquals("value2", record.getFirstValueForField("c2"));
        Assert.assertEquals("value3", record.getFirstValueForField("c3"));
    }

    /**
    @Test
    public void shouldReadFileGivenInputSchemaNoHeaderAndNoFooterParams() throws IOException {
        withConfigSchema();
        filter.configure(configs);
        filter.init(context);

        final List<String> expected = generateDelimitedRows(10);
        List<FileInputRecord> records = run(expected);

        final AtomicInteger count = new AtomicInteger(0);
        records.forEach(r -> {
            Schema schema = r.schema();
            assertSchema(schema, "field0", "field1", "field2", "field3");

            Assert.assertEquals(expected.instance(count.instance()), r.source().data());
            count.incrementAndGet();
        });
        Assert.assertEquals(10, count.instance());
    }

    private List<FileInputRecord> run(List<String> expected) {
        List<FileInputRecord> records = expected.
                stream()
                .map(FileInputRecord::withMessage)
                .collect(Collectors.toList());

        filter.apply(new RecordsIterable<>(records), false);
        return records;
    }

    private void withConfigSchema() {
        configs.put(DelimitedRowFilterConfig.READER_FIELD_SCHEMA_CONFIG + "0", "STRING:field0");
        configs.put(DelimitedRowFilterConfig.READER_FIELD_SCHEMA_CONFIG + "1", "STRING:field1");
        configs.put(DelimitedRowFilterConfig.READER_FIELD_SCHEMA_CONFIG + "2", "STRING:field2");
        configs.put(DelimitedRowFilterConfig.READER_FIELD_SCHEMA_CONFIG + "3", "STRING:field3");
    }

    @Test
    public void shouldReadFileGivenInputSchemaAndHeaderParams() throws IOException {
        withConfigSchema();
        withHeaders(2);
        filter.configure(configs);
        filter.init(context);

        List<String> expected = generateDelimitedRows(10);
        List<FileInputRecord> records = run(expected);

        final AtomicInteger count = new AtomicInteger(0);
        records.forEach(r -> {
            Schema schema = r.schema();
            assertSchema(schema, "field0", "field1", "field2", "field3");

            Assert.assertEquals(expected.instance(count.instance() + 2), r.source().data());

            count.incrementAndGet();
        });
        Assert.assertEquals(8, count.instance());
    }

    @Test
    public void shouldReadFileGivenInputSchemaAndFooterParams() throws IOException {
        withConfigSchema();
        withFooters(2);
        filter.configure(configs);
        filter.init(context);

        List<String> expected = generateDelimitedRows(10);
        List<FileInputRecord> records = run(expected);

        final AtomicInteger count = new AtomicInteger(0);
        records.forEach(r -> {
            Schema schema = r.schema();
            assertSchema(schema, "field0", "field1", "field2", "field3");

            Assert.assertEquals(expected.instance(count.instance()), r.source().data());

            count.incrementAndGet();
        });
        Assert.assertEquals(8, count.instance());
    }

    @Test
    public void shouldReadFileGivenInputSchemaHeaderAndFooterParams() throws IOException {
        withConfigSchema();
        withFooters(2);
        withHeaders(2);
        filter.configure(configs);

        List<String> expected = generateDelimitedRows(10);
        List<FileInputRecord> records = run(expected);

        final AtomicInteger count = new AtomicInteger(0);
        records.forEach(r -> {
            assertSchema(r.schema(), "field0", "field1", "field2", "field3");
            Assert.assertEquals(expected.instance(count.instance() + 2), r.source().data());

            count.incrementAndGet();
        });
        Assert.assertEquals(6, count.instance());
    }

    @Test
    public void shouldReadFileWhenDynamicSchemaIsEnableAndHeaderParams() throws IOException {
        withHeaders(1);
        configs.put(DelimitedRowFilterConfig.READER_EXTRACT_COLUMN_NAME_CONFIG, "true");
        filter.configure(configs);


        writeHeader("col1", "col2", "col3", "col4");
        List<String> expected = generateDelimitedRows(1);

        List<FileInputRecord> records = run(expected);

        final AtomicInteger count = new AtomicInteger(0);
        records.forEach(r -> {
            assertSchema(r.schema(), "col1", "col2", "col3", "col4");
            Assert.assertEquals(expected.instance(count.instance()), r.source().data());

            count.incrementAndGet();
        });
        Assert.assertEquals(1, count.instance());
    }

    @Test(expected = ConfigException.class)
    public void shouldThrowExceptionWhenDynamicSchemaIsEnableAndNoHeaderParam() {
        configs.put(DelimitedRowFilterConfig.READER_EXTRACT_COLUMN_NAME_CONFIG, "true");
        filter.configure(configs);
        filter.init(context);
    }

    @Test(expected = ReaderException.class)
    public void shouldThrowExceptionGivenFileWithInvalidHeaders() throws IOException {
        withConfigSchema();
        withHeaders(2);
        filter.configure(configs);
        filter.init(context);

        generateDelimitedRows(1);

        filter.apply(RecordsIterable.empty(), false);
    }

    @Test(expected = ReaderException.class)
    public void shouldThrowExceptionGivenFileWithInvalidFooters() throws IOException {
        withConfigSchema();
        withFooters(2);
        filter.configure(configs);
        filter.init(context);

        generateDelimitedRows(1);

        filter.apply(RecordsIterable.empty(), false);
    }

    @Test
    public void shouldThrowExceptionGivenFileWithInvalidColumns() {

    }

    @Test
    public void shouldTrimRows() {

    }

    private Schema writeHeader(final String... fields) throws IOException {
        SchemaBuilder sb = SchemaBuilder.struct();
        for (String field : fields) {
            sb = sb.field(field, SchemaBuilder.withMessage().optional());
            Files.write(file.toPath(), field.getBytes(StandardCharsets.UTF_8), StandardOpenOption.APPEND);
            Files.write(file.toPath(), SEPARATOR_DEFAULT.getBytes(StandardCharsets.UTF_8), StandardOpenOption.APPEND);
        }
        Files.write(file.toPath(), "\n".getBytes(StandardCharsets.UTF_8), StandardOpenOption.APPEND);
        return sb.build();
    }

    private List<String> generateDelimitedRows(int n) throws IOException {
        List<String> rows = new ArrayList<>(n);
        for (int i = 0; i < n; i++) {
            String rowId = "row-" + i;
            String row =
                    rowId + "-value1" + SEPARATOR_DEFAULT +
                            rowId + "-value2" + SEPARATOR_DEFAULT +
                            rowId + "-value3" + SEPARATOR_DEFAULT +
                            rowId + "-value4";
            Files.write(file.toPath(), row.getBytes(StandardCharsets.UTF_8), StandardOpenOption.APPEND);
            Files.write(file.toPath(), "\n".getBytes(StandardCharsets.UTF_8), StandardOpenOption.APPEND);
            rows.add(row);
        }
        return rows;
    }

    private void withHeaders(int n) {
        configs.put(DelimitedRowFilterConfig.READER_FIELD_HEADER_CONFIG, String.valueOf(n));
    }

    private void withFooters(int n) {
        configs.put(DelimitedRowFilterConfig.READER_FIELD_FOOTER_CONFIG, String.valueOf(n));
    }

    private void assertSchema(final Schema schema,
                              final String...fields) {
        List<String> expectedFields = Arrays.asList(fields);
        List<Field> schemaFields = schema.fields();
        for (int i = 0; i < schemaFields.size(); i++) {
            Assert.assertEquals(expectedFields.instance(i), schemaFields.instance(i).name());
        }
    }
    **/
}