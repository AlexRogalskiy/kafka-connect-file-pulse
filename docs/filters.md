# Filters

These filters are available for use with Kafka Connect File Pulse:

| Filter | Description |
|---     | --- |
| [AppendFilter](#appendfilter) | Appends one or more values to an existing or non-existing array field  |
| [ConvertFilter](#convertfilter)  | Converts a message field's value to a specific type |
| [DelimitedRowFilter](#delimitedrowfilter)  | Parses a message field's value containing columns delimited by a separator into a struct |
| [GrokFilter](#grokfilter)  | Parses an unstructured message field's value to a struct by combining Grok patterns |
| [GroupRowFilter](#grouprowfilter)  | Regroups multiple following messages into a single message by composing a grouping key|
| [JSONFilter](#jsonfilter)  | Unmarshallings a JSON message field's value to a complex struct |
| [MultiRowFilter](#multirowfilter)  | Combines following message lines into single one by combining patterns |
| [RenameFilter](#renamefilter)  | Renames a message field |
| [SplitFilter](#splitfilter)  | Splits a message field's value to array |

## AppendFilter

The following provides usage information for : `io.streamthoughts.kafka.connect.filepulse.filter.AppendFilter`

### Configuration

| Configuration |   Description |   Type    |   Default |   Importance  |
| --------------| --------------|-----------| --------- | ------------- |
| `field` | The field to append    | string | *-* | high |
| `value` | The value to be appended    | string | *-* | high |
| `overwrite` | Is existing field should be overwrite    | boolean | *false* | high |

### Example

```
```

## ConvertFilter

The following provides usage information for : `io.streamthoughts.kafka.connect.filepulse.filter.ConvertFilter`

### Configuration

| Configuration |   Description |   Type    |   Default |   Importance  |
| --------------| --------------|-----------| --------- | ------------- |
| `field` | The field to convert    | string | *-* | high |
| `type` | The type field must be converted to  | string | *,* | high |
| `ignoreMissing` | If true and field does not exist the filter will be apply successfully without modifying the data. If field is null the schema will be modified. | boolean | *true* | high |

### Example

```
```

## DelimitedRowFilter

The following provides usage information for : `io.streamthoughts.kafka.connect.filepulse.filter.DelimitedRowFilter`

The delimited-row filter can be used to parse and stream delimited row files (i.e CSV) into Kafka.
Each row is parsed and published into a configured topic as a single Kafka data.

### Configuration

| Configuration |   Description |   Type    |   Default |   Importance  |
| --------------| --------------|-----------| --------- | ------------- |
|`separator` | The character used as a delimiter/separator between each value | string |*;* |  high |
|`trimColumn` | Remove the leading and trailing whitespaces from all columns. |  boolean | *false* | low |
|`extractColumnName` | Define the field from which the schema should be detected (all columns will be of type 'string') | string | | high |
|`autoGenerateColumnNames` | Define whether column names should autogenerated or not (column names will of the form 'column1, column2') | *true* | boolean | high |
|`columns` | Define the list of column names in order they appear in each row. columns must be in the form of TYPE:NAME | string | | high |

### Example

```
```

## GrokFilter

The following provides usage information for : `io.streamthoughts.kafka.connect.filepulse.filter.GrokFilter`

The grok filter allows you to parse unstructured data like applications logs to extract structured and meaningful data fields.

**Regular Expressions**
Grok are built on top of on regular expressions, so you can use any regular expressions as well to define your own patterns.
The regular expression library is [Joni](https://github.com/jruby/joni), a Java port of Oniguruma regexp library

### Configuration

| Configuration |   Description |   Type    |   Default |   Importance  |
| --------------| --------------|-----------| --------- | ------------- |
| `namedCapturesOnly` | If true, only store named captures from grok. | boolean | *true* | high |
| `matches` | The Grok pattern to match. | string | *-* | high |
| `overwrite` | The fields to overwrite.    | list | moderate |
| `patternDefinitions` | Custom pattern definitions. | list | *-* | low |
| `patternsDir` | List of user-defined pattern directories | string | *-* | low |
| `source` | The input field on which to apply the filter  | string | *message* | medium |

### Example

```
```

## GroupRowFilter

The following provides usage information for : `io.streamthoughts.kafka.connect.filepulse.filter.GroupRowFilter`

### Configuration

| Configuration |   Description |   Type    |   Default |   Importance  |
| --------------| --------------|-----------| --------- | ------------- |
| `fields` | List of fields used to regroup records | list | high |
| `max.buffered.records` | The maximum number of records to group (default : -1).| integer | *-1* | high |
| `target` | The target array field to put the grouped field | integer | *records* | high |

### Example

```
```

## JSONFilter

The following provides usage information for : `io.streamthoughts.kafka.connect.filepulse.filter.JSONFilter`


The JSON filter parses an input json field.

### Configuration

| Configuration |   Description |   Type    |   Default |   Importance  |
| --------------| --------------|-----------| --------- | ------------- |
| `overwrite` | The fields to overwrite.    | list | *-* | medium |
| `source` | The input field on which to apply the filter  | string | *message* | medium |
| `target` | he target field to put the parsed JSON data  | string | *-* | high |

### Example

```
filters=MyJsonFilter
filters.MyJsonFilter.type=io.streamthoughts.kafka.connect.filepulse.filter.JSONFilter
filters.MyJsonFilter.source=message
filters.MyJsonFilter.target=payload
```

## MultiRowFilter

The following provides usage information for : `io.streamthoughts.kafka.connect.filepulse.filter.MultiRowFilter`


The multirow filter joins multiple lines into a single defaultStruct using a regex pattern.
For example, this filter can be used for joining Java exception and stacktrace messages 

### Configuration

| Configuration |   Description |   Type    |   Default |   Importance  |
| --------------| --------------|-----------| --------- | ------------- |
| `negate` | Negate the regexp pattern (if not matched)."   | boolean | *-* | medium |
| `pattern` | The pattern to match multiline  | string | *-* | high |
| `patternDefinitions` | Custom pattern definitions. | list | *-* | low |
| `patternsDir` | List of user-defined pattern directories | string | *-* | low |
| `separator` | The character to be used to concat multi lines  | string | "\\n" | high |

### Example

```
filters=StackTraceMultiRowFilter
filters.StackTraceMultiRowFilter.type=io.streamthoughts.kafka.connect.filepulse.filter.MultiRowFilter
filters.StackTraceMultiRowFilter.negate=false
filters.StackTraceMultiRowFilter.pattern=^[\t]
```

## RenameFilter

The following provides usage information for : `io.streamthoughts.kafka.connect.filepulse.filter.RenameFilter`

### Configuration

| Configuration |   Description |   Type    |   Default |   Importance  |
| --------------| --------------|-----------| --------- | ------------- |
| `field` | The field to rename | string | *-* | high |
| `target` | The target name | string | *-* | high |
| `ignoreMissing` | If true and field does not exist the filter will be apply successfully without modifying the data. If field is null the schema will be modified. | boolean | *true* | high |

### Example

```
```

## SplitFilter

The following provides usage information for : `io.streamthoughts.kafka.connect.filepulse.filter.SplitFilter`

### Configuration

| Configuration |   Description |   Type    |   Default |   Importance  |
| --------------| --------------|-----------| --------- | ------------- |
| `split` | Split a message field's value to array    | string | *-* | high |
| `separator` | The separator used for splitting a withMessage field's value to array  | string | *,* | high |
| `target` | he target field to put the parsed JSON data  | string | *-* | high |

{% include_relative plan.md %}