<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kafka Connect File Pulse ‚Äì Connect File Pulse | Source connector for Apache Kafka</title>
    <link>https://streamthoughts.github.io/kafka-connect-file-pulse/</link>
    <description>Recent content in Connect File Pulse | Source connector for Apache Kafka on Kafka Connect File Pulse</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="https://streamthoughts.github.io/kafka-connect-file-pulse/index.xml" rel="self" type="application/rss+xml" />
    
    
      
      
    
    
    <item>
      <title>Blog: Connect FilePulse 2.2 is Released üöÄ</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/blog/2021/08/10/connect-filepulse-2.2-is-released/</link>
      <pubDate>Tue, 10 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/blog/2021/08/10/connect-filepulse-2.2-is-released/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;This new release brings new capabilities and several bug fixes and improvements that make ConnectFilePulse still the more powerful Kafka Connect-based solution for processing files.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;support-for-filelisting-process-delegation&#34;&gt;Support for file-listing process delegation&lt;/h2&gt;
&lt;p&gt;From the very beginning, Connect FilePulse was designed differently from most other Kafka connectors used for file processing.
For example, we chose to use a single thread called the `FileSystemMonitorThread, managed by the connector instance, to scan the files to be processed and to apply the configured file cleanup policy.
Then, when new files are detected on the file system, the connector instance triggers a task reconfiguration to distribute the files to be processed among the tasks.&lt;/p&gt;
&lt;p&gt;Although this design offers many advantages it also brings some limitations that may make the connector less suitable for some scenarios,
such as processing a very large number of small files that would be created quickly on the file system.&lt;/p&gt;
&lt;p&gt;This limitation is mainly due to the fact that every time a task reconfiguration is triggered,
Kafka Connect needs to stop and restart all tasks of our connector using the internal Kafka rebalance protocol.
Thus, the connector may have some scalability issues if it is necessary to reconfigure tasks every second because new files have been created on the local filesystem.&lt;/p&gt;
&lt;p&gt;To support such a scenario, Connect FilePulse 2.2.0 brings a new feature to delegate the file listing process to the connector&#39;s tasks
This new feature can be enabled by setting the connector&#39;s property &lt;code&gt;fs.listing.task.delegation.enabled&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When enabled, each task will scan the filesystem using the &lt;code&gt;fs.listing.class&lt;/code&gt; passed through the connector&#39;s configuration.
In addition, a dedicated &lt;code&gt;TaskPartitioner&lt;/code&gt; is used to partition each file to a single task using the murmur2 hash algorithm.
Finally, the cleanup policy passed through the connector&#39;s configuration is still executed by the &lt;code&gt;FileSystemMonitorThread&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;taskpartitioner&#34;&gt;TaskPartitioner&lt;/h2&gt;
&lt;p&gt;Additionally, Connect FilePulse 2.2.0 introduces a new pluggable interface called &lt;code&gt;TaskPartitioner&lt;/code&gt; used to partition files among the connector&#39;s tasks.
The connector ships with two built-in implementations: the &lt;code&gt;DefaultTaskPartitioner&lt;/code&gt; that spreads files evenly among the tasks and the &lt;code&gt;HashByURITaskPartitioner&lt;/code&gt; that partitions each file based on its URI.&lt;/p&gt;
&lt;h2 id=&#34;improved-support-for-xml&#34;&gt;Improved support for XML&lt;/h2&gt;
&lt;p&gt;Connect FilePulse 2.2.0 adds various improvements for XML support.
So now, when using Connect FilePulse with the &lt;code&gt;LocalXMLFileInputReader&lt;/code&gt; you can enable the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;reader.xml.parser.validating.enabled=true&lt;/code&gt;: To specify that the XML parser should validate documents as they are parsed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;reader.xml.parser.namespace.aware.enabled=true&lt;/code&gt;: To specify that the XML parser should provide support for XML namespaces.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;reader.xml.exclude.empty.elements&lt;/code&gt;: To specify that the reader should automatically exclude elements having no field.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Furthermore, dynamic schema resolution has been improved when processing complex XML documents and more especially when handling documents with elements containing arrays.&lt;/p&gt;
&lt;h2 id=&#34;improved-lastmodifiedfilelistfilter&#34;&gt;Improved LastModifiedFileListFilter&lt;/h2&gt;
&lt;p&gt;Finally, this new release enhances the &lt;code&gt;LastModifiedFileListFilter&lt;/code&gt; to allow configuring the maximum age in milliseconds of a file to be eligible for processing.
For this, you can use the new connector&#39;s configuration property: &lt;code&gt;file.filter.maximum.age.ms&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;full-release-notes&#34;&gt;Full Release Notes&lt;/h2&gt;
&lt;p&gt;Connect File Pulse 2.2 can be downloaded from the &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/releases/tag/v2.2.0&#34;&gt;GitHub Releases Page&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;new-features&#34;&gt;New Features&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/5bc2024&#34;&gt;5bc2024&lt;/a&gt; feat(plugin): allow excluding from processing files based on maximum age in ms (#161)
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/a3ef5db&#34;&gt;a3ef5db&lt;/a&gt; feat(filesystems): improve XMLFileInputIterator to allow excluding empty element (#159)
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/b00ef42&#34;&gt;b00ef42&lt;/a&gt; feat(scripts): add arg to specify number of connect workers
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/ef8fbc3&#34;&gt;ef8fbc3&lt;/a&gt; feat(plugin): add support for task file listing delegation
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/137c1a6&#34;&gt;137c1a6&lt;/a&gt; feat(filesystems): enhance XMLFileInputIterator with new config props&lt;/p&gt;
&lt;h3 id=&#34;improvements--bugfixes&#34;&gt;Improvements &amp;amp; Bugfixes&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/a693b52&#34;&gt;a693b52&lt;/a&gt; fix(plugin): fix FilePulseSourceConnector should raise an error when FileSystemMonitorThread crash
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/c6f1f13&#34;&gt;c6f1f13&lt;/a&gt; fix(api): fix empty document removing in XMLFileInputIterator
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/c0cc249&#34;&gt;c0cc249&lt;/a&gt; fix(api): improve support for XML by adding capabilities to merge schemas (#160)&lt;/p&gt;
&lt;h3 id=&#34;docs&#34;&gt;Docs&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/0c92901&#34;&gt;0c92901&lt;/a&gt; docs(site): fix page date and css
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/7000b84&#34;&gt;7000b84&lt;/a&gt; docs(site): add release note for 2.1.0&lt;/p&gt;
&lt;h3 id=&#34;subtasks&#34;&gt;Sub-Tasks&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/4f0add2&#34;&gt;4f0add2&lt;/a&gt; project(issue): add github stale bot config
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/34ab52a&#34;&gt;34ab52a&lt;/a&gt; fix(scripts): update docker-compose for debug
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/a2fb4a5&#34;&gt;a2fb4a5&lt;/a&gt; sub-task(plugin): add new interface TaskPartitioner
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/bbeebda&#34;&gt;bbeebda&lt;/a&gt; subtask(plugin): add new interface FileURIProvider
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/e2825c0&#34;&gt;e2825c0&lt;/a&gt; build(deps): bump commons-compress from 1.20 to 1.21&lt;/p&gt;
&lt;p&gt;If you enjoyed reading this post, check out Connect FilePulse at GitHub and give us a ‚≠ê!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Connect FilePulse 2.1 is Available üöÄ</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/blog/2021/08/04/connect-filepulse-2.1-is-available/</link>
      <pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/blog/2021/08/04/connect-filepulse-2.1-is-available/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;This new release contains a number of bug fixes and improvements that make ConnectFilePulse more stable and resilient in production.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;full-release-notes&#34;&gt;Full Release Notes&lt;/h2&gt;
&lt;p&gt;Connect File Pulse 2.1 can be downloaded from the &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/releases/tag/v2.1.0&#34;&gt;GitHub Releases Page&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;improvements--bugfixes&#34;&gt;Improvements &amp;amp; Bugfixes&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/f1c071e&#34;&gt;f1c071e&lt;/a&gt; refactor(plugin): change default value of &lt;code&gt;offset.attributes.string&lt;/code&gt; to uri (#154)
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/d8aac2b&#34;&gt;d8aac2b&lt;/a&gt; refactor(plugin): enhance error handling when file do not exist anymore
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/8a17051&#34;&gt;8a17051&lt;/a&gt; fix(plugin): fix ClassCastException when offset.attributes.string=inode (#153)
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/defee21&#34;&gt;defee21&lt;/a&gt; fix(plugin): remove duplicate log when closing file iterator
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/7897bcb&#34;&gt;7897bcb&lt;/a&gt; fix(plugin): improve DefaultFileSystemMonitor to avoid scheduling files that may be cleanup by remaining tasks (#152)
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/28456db&#34;&gt;28456db&lt;/a&gt; fix(plugin): fix task must close resources on error while starting
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/ddf8b86&#34;&gt;ddf8b86&lt;/a&gt; fix(api): fix DeadLock on KafkaStateBackingStore
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/370a1d6&#34;&gt;370a1d6&lt;/a&gt; fix(filesystems): fix FileSystemMonitorThread should not fail if file metadata cannot be retrieved (#150)
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/e9462c8&#34;&gt;e9462c8&lt;/a&gt; fix(plugin): fix NPE using version 2.0 with KafkaFileObjectStateBackingStore (#149)&lt;/p&gt;
&lt;h3 id=&#34;docs&#34;&gt;Docs&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/1b72a96&#34;&gt;1b72a96&lt;/a&gt; docs(site): fix syntax for &amp;lsquo;exists&amp;rsquo; ScEL Built-in Function (#148)
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/7da7e41&#34;&gt;7da7e41&lt;/a&gt; docs(site): fix documentation error for StateBackingStore (#147)&lt;/p&gt;
&lt;h3 id=&#34;subtasks&#34;&gt;Sub-Tasks&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/ec70611&#34;&gt;ec70611&lt;/a&gt; build(deps): bump commons-io from 2.5 to 2.7
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/aa62022&#34;&gt;aa62022&lt;/a&gt; fix(script): fix docker-compose for debugging
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/2723516&#34;&gt;2723516&lt;/a&gt; refactor(build): add mvn profiles for different storages
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/c1da2f8&#34;&gt;c1da2f8&lt;/a&gt; refactor(build): improve makefile and add utility script for debugging
&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/54ba1e5&#34;&gt;54ba1e5&lt;/a&gt; build(maven): add meta info&lt;/p&gt;
&lt;p&gt;If you enjoyed reading this post, check out Connect FilePulse at GitHub and give us a ‚≠ê!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Connect FilePulse 2.0 is Available üöÄ</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/blog/2021/06/10/connect-filepulse-2.0-is-available/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/blog/2021/06/10/connect-filepulse-2.0-is-available/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Connect FilePulse 2.0 is finally here! Here is an overview of what is new:&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;supported-cloud-storage&#34;&gt;Supported Cloud Storage&lt;/h2&gt;
&lt;p&gt;Previously, Connect FilePulse was designed to provide direct integration between legacy systems and Apache Kafka.
But, it could be only used to process and integrate data records from the local filesystem on which the connector was deployed.&lt;/p&gt;
&lt;p&gt;As more and more organizations move from on-premises to cloud infrastructure, we&#39;ve seen a growing demand from developers for the connector to support cloud storage.&lt;/p&gt;
&lt;p&gt;Connect FilePulse 2.0 brings you the capabilities for reading files across different storage systems.
Using a single Kafka Connect Source Connector you can now read files from the local filesystem, Amazon S3, Azure Blob Storage and Google Cloud Storage.&lt;/p&gt;
&lt;p&gt;In addition, the connector supports a variety of formats equally for all storage systems, e.g., text files, CSV, XML, JSON, Avro, etc.
At the same time, you can still benefit from the powerful &lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters-chain-definition/&#34;&gt;processing-filters&lt;/a&gt; mechanism of Connect FilePulse to process data records as they are read by the connector.&lt;/p&gt;
&lt;p&gt;For example, here is the configuration for reading CSV object files from an Amazon S3 bucket.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;name=connect-file-pulse-amazon-s3-csv
connector.class=io.streamthoughts.kafka.connect.filepulse.source.FilePulseSourceConnector
topic=connect-filepulse-csv-data-records
tasks.max=1

fs.listing.class=io.streamthoughts.kafka.connect.filepulse.fs.AmazonS3FileSystemListing
fs.listing.interval.ms=10000
fs.listing.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.RegexFileListFilter
file.filter.regex.pattern=.*\\.csv$

fs.cleanup.policy.class=io.streamthoughts.kafka.connect.filepulse.clean.LogCleanupPolicy

aws.access.key.id=xxxxxxxxx
aws.secret.access.key=xxxxxxxxx
aws.s3.region=eu-west-3
aws.s3.bucket.name=connect-filepulse

tasks.reader.class=io.streamthoughts.kafka.connect.filepulse.fs.reader.AmazonS3RowFileInputReader

skip.headers=1
offset.attributes.string=uri

filters=ParseLine
filters.ParseLine.type=io.streamthoughts.kafka.connect.filepulse.filter.DelimitedRowFilter
filters.ParseLine.extractColumnName=headers
filters.ParseLine.trimColumn=true
filters.ParseLine.separator=;
tasks.file.status.storage.bootstrap.servers=kafka101:9092
tasks.file.status.storage.topic=connect-file-pulse-status
tasks.file.status.storage.topic.partitions=10
tasks.file.status.storage.topic.replication.factor=1
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;autocreate-internal-topic&#34;&gt;Auto-create Internal Topic&lt;/h2&gt;
&lt;p&gt;By default, Connect FilePulse uses the internal topic &lt;code&gt;connect-file-pulse-status&lt;/code&gt; to track the current state of each file
being scheduled and processed by tasks. This allows you to deploy Connect FilePulse is a distributed Kafka Connect cluster with each worker only processing a subset of files.&lt;/p&gt;
&lt;p&gt;In version 2.0, this topic is will be automatically created by the connector if it doesn&#39;t already exist. You can configure the number of partitions, as well as, the replication factor of this topic using the
new properties &lt;code&gt;tasks.file.status.storage.topic.partitions&lt;/code&gt; and &lt;code&gt;tasks.file.status.storage.topic.replication.factor&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;inmemoryfileobjectstatebackingstore&#34;&gt;InMemoryFileObjectStateBackingStore&lt;/h2&gt;
&lt;p&gt;In version 2.0, we provide a new property &lt;code&gt;tasks.file.status.storage.class&lt;/code&gt; that can be used to specify the class implementing the &lt;code&gt;FileObjectStateBackingStore&lt;/code&gt; interface
to be used for storing the status of each file. By default, Connect FilePulse uses the kafka-based implementation called &lt;code&gt;i.s.k.c.f.state.KafkaFileObjectStateBackingStore&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But, in some context, it may be not necessary to deploy Connect FilePulse in distributed mode and this implementation can lead to additional costs if, for example, you are using a fully-managed Apache Kafka service.
So now, we also provide the &lt;code&gt;i.s.k.c.f.state.InMemoryFileObjectStateBackingStore&lt;/code&gt; implementation to only keep file status in-memory.&lt;/p&gt;
&lt;h2 id=&#34;improved-scalability&#34;&gt;Improved Scalability&lt;/h2&gt;
&lt;p&gt;Connect FilePulse can be used to integrate a very large number of files in parallel.
Unfortunately, too many files to process can result in a too-large message to produce in Kafka for configuring tasks (i.e. &lt;code&gt;connect-config&lt;/code&gt;).
To solve this blocking issue, in version 2.0, we have added the new property &lt;code&gt;max.scheduled.files&lt;/code&gt; to limit the maximum number of files that can be scheduled at the same time (Default is &lt;code&gt;1000&lt;/code&gt;).&lt;/p&gt;
&lt;h2 id=&#34;improved-grok-expression&#34;&gt;Improved Grok Expression&lt;/h2&gt;
&lt;p&gt;In a previous version, Connect FilePulse has brought the support for Grok expressions to parse data.
Since this mechanism has been migrated to a new dedicated project &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-transform-grok&#34;&gt;kafka-connect-transform-grok&lt;/a&gt; in order to be able to use Grok expressions with Kafka Connect&#39;s a SMTs.
Now, Connect FilePulse directly depends on that project to provide the &lt;code&gt;GrokFilter&lt;/code&gt; with a unified configuration.&lt;/p&gt;
&lt;h2 id=&#34;full-release-notes&#34;&gt;Full Release Notes&lt;/h2&gt;
&lt;p&gt;Connect File Pulse 2.0 can be downloaded from the &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/releases/tag/v2.0.0&#34;&gt;GitHub Releases Page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Members of the open-source community who appear in these release notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;@at0dd&lt;/li&gt;
&lt;li&gt;@qgeffard&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for your valuable contributions!&lt;/p&gt;
&lt;h3 id=&#34;features&#34;&gt;Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/13eed7b&#34;&gt;13eed7b&lt;/a&gt; feat(plugin): add support for auto-creating the internal topic used by ConnectFilePulse (#139)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/5c88877&#34;&gt;5c88877&lt;/a&gt; feat(plugin): add InMemoryStateBackingStore for tracking status of file objects (#138)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/52adca9&#34;&gt;52adca9&lt;/a&gt; feat(filesystems): add support for Google Cloud Storage (#121)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/7b49b81&#34;&gt;7b49b81&lt;/a&gt; feat(plugin): add new property max.scheduled.files (#122) (#123)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/390ad82&#34;&gt;390ad82&lt;/a&gt; feat(filesystems): add support for AWS S3 (#111)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/92e3341&#34;&gt;92e3341&lt;/a&gt; feat(filesystems): add support for Azure Blob Storage (#112)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/685618a&#34;&gt;685618a&lt;/a&gt; refactor(filters): migrate GrokFilter to use classes from grok-transformer (#118)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;improvements--bugfixes&#34;&gt;Improvements &amp;amp; Bugfixes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/6ef5162&#34;&gt;6ef5162&lt;/a&gt; fix(api): fix decimal numbers not being correctly parsed (#142)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/6c779af&#34;&gt;6c779af&lt;/a&gt; refactor(filesystems): make cleanup policy storage aware&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/d13c236&#34;&gt;d13c236&lt;/a&gt; fix(filesystems): make compression codec more robust to encoding&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/7d2ddac&#34;&gt;7d2ddac&lt;/a&gt; docs(site): fix DateFilter formats config&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/e222414&#34;&gt;e222414&lt;/a&gt; fix(api): change digest value to string&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;subtasks&#34;&gt;SubTasks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/1658d35&#34;&gt;1658d35&lt;/a&gt; refactor(api/filesystems): move FileInputIterator implementation to commons-fs&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/06385b3&#34;&gt;06385b3&lt;/a&gt; refactor(filesystems): add module filepulse-commons-fs&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/57da04c&#34;&gt;57da04c&lt;/a&gt; subtask(all): refactor FilePulse API to support remote storages (#100)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/ee4acad&#34;&gt;ee4acad&lt;/a&gt; add github workflow&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/a3eb908&#34;&gt;a3eb908&lt;/a&gt; build(all): update to java 11&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/commit/98eb51f&#34;&gt;98eb51f&lt;/a&gt; build(mvn): add maven-wrapper&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;braking-changes&#34;&gt;Braking changes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Configurations for Connect FilePulse 1.x  is not compatible with the version 2.x.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you enjoyed reading this post, check out Connect FilePulse at GitHub and give us a ‚≠ê!&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
