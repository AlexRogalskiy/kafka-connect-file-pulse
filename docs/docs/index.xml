<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kafka Connect File Pulse â€“ Documentation</title>
    <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/</link>
    <description>Recent content in Documentation on Kafka Connect File Pulse</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="https://streamthoughts.github.io/kafka-connect-file-pulse/docs/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Installation</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/developer-guide/installation/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/developer-guide/installation/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Connect FilePulse&lt;/strong&gt; can be installed either from &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/releases&#34;&gt;GitHub Releases Page&lt;/a&gt; or from &lt;a href=&#34;https://www.confluent.io/hub/streamthoughts/kafka-connect-file-pulse&#34;&gt;Confluent Hub&lt;/a&gt;.&lt;/p&gt;


&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Caution&lt;/h4&gt;
You should note that the connector downloaded from Confluent Hub may not reflect the latest available version.
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Confluent Hub CLI installation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use the &lt;a href=&#34;https://docs.confluent.io/current/confluent-hub/client.html&#34;&gt;Confluent Hub client&lt;/a&gt; to install this connector with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;confluent-hub install streamthoughts/kafka-connect-file-pulse:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Download Installation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Download the distribution ZIP file for the latest available version.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example :&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#204a87&#34;&gt;export&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;VERSION&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;1.3.0
curl -sSL https://github.com/streamthoughts/kafka-connect-file-pulse/releases/download/v&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;/streamthoughts-kafka-connect-file-pulse-&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Extract it into one of the directories that is listed on the &lt;code&gt;plugin.path&lt;/code&gt; worker configuration property.&lt;/p&gt;
&lt;p&gt;You can also use the Confluent Hub CLI for installing it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ confluent-hub install --no-prompt streamthoughts-kafka-connect-file-pulse-&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Important&lt;/h4&gt;
When you run Connect workers in &lt;strong&gt;distributed mode&lt;/strong&gt;, the connector-plugin must be installed &lt;strong&gt;on each of machines&lt;/strong&gt; running Kafka Connect.
&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Installation</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/developer-guide/installation/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/developer-guide/installation/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Connect FilePulse&lt;/strong&gt; can be installed either from &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/releases&#34;&gt;GitHub Releases Page&lt;/a&gt; or from &lt;a href=&#34;https://www.confluent.io/hub/streamthoughts/kafka-connect-file-pulse&#34;&gt;Confluent Hub&lt;/a&gt;.&lt;/p&gt;


&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Caution&lt;/h4&gt;
You should note that the connector downloaded from Confluent Hub may not reflect the latest available version.
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Confluent Hub CLI installation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use the &lt;a href=&#34;https://docs.confluent.io/current/confluent-hub/client.html&#34;&gt;Confluent Hub client&lt;/a&gt; to install this connector with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;confluent-hub install streamthoughts/kafka-connect-file-pulse:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Download Installation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Download the distribution ZIP file for the latest available version.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example :&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#204a87&#34;&gt;export&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;VERSION&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;1.3.0
curl -sSL https://github.com/streamthoughts/kafka-connect-file-pulse/releases/download/v&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;/streamthoughts-kafka-connect-file-pulse-&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Extract it into one of the directories that is listed on the &lt;code&gt;plugin.path&lt;/code&gt; worker configuration property.&lt;/p&gt;
&lt;p&gt;You can also use the Confluent Hub CLI for installing it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ confluent-hub install --no-prompt streamthoughts-kafka-connect-file-pulse-&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Important&lt;/h4&gt;
When you run Connect workers in &lt;strong&gt;distributed mode&lt;/strong&gt;, the connector-plugin must be installed &lt;strong&gt;on each of machines&lt;/strong&gt; running Kafka Connect.
&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Installation</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/developer-guide/installation/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/developer-guide/installation/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Connect FilePulse&lt;/strong&gt; can be installed either from &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/releases&#34;&gt;GitHub Releases Page&lt;/a&gt; or from &lt;a href=&#34;https://www.confluent.io/hub/streamthoughts/kafka-connect-file-pulse&#34;&gt;Confluent Hub&lt;/a&gt;.&lt;/p&gt;


&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Caution&lt;/h4&gt;
You should note that the connector downloaded from Confluent Hub may not reflect the latest available version.
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Confluent Hub CLI installation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use the &lt;a href=&#34;https://docs.confluent.io/current/confluent-hub/client.html&#34;&gt;Confluent Hub client&lt;/a&gt; to install this connector with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;confluent-hub install streamthoughts/kafka-connect-file-pulse:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Download Installation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Download the distribution ZIP file for the latest available version.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example :&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#204a87&#34;&gt;export&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;VERSION&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;1.3.0
curl -sSL https://github.com/streamthoughts/kafka-connect-file-pulse/releases/download/v&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;/streamthoughts-kafka-connect-file-pulse-&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Extract it into one of the directories that is listed on the &lt;code&gt;plugin.path&lt;/code&gt; worker configuration property.&lt;/p&gt;
&lt;p&gt;You can also use the Confluent Hub CLI for installing it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ confluent-hub install --no-prompt streamthoughts-kafka-connect-file-pulse-&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Important&lt;/h4&gt;
When you run Connect workers in &lt;strong&gt;distributed mode&lt;/strong&gt;, the connector-plugin must be installed &lt;strong&gt;on each of machines&lt;/strong&gt; running Kafka Connect.
&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Installation</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/developer-guide/installation/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/developer-guide/installation/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Connect FilePulse&lt;/strong&gt; can be installed either from &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/releases&#34;&gt;GitHub Releases Page&lt;/a&gt; or from &lt;a href=&#34;https://www.confluent.io/hub/streamthoughts/kafka-connect-file-pulse&#34;&gt;Confluent Hub&lt;/a&gt;.&lt;/p&gt;


&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Caution&lt;/h4&gt;
You should note that the connector downloaded from Confluent Hub may not reflect the latest available version.
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Confluent Hub CLI installation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use the &lt;a href=&#34;https://docs.confluent.io/current/confluent-hub/client.html&#34;&gt;Confluent Hub client&lt;/a&gt; to install this connector with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;confluent-hub install streamthoughts/kafka-connect-file-pulse:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Download Installation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Download the distribution ZIP file for the latest available version.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example :&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#204a87&#34;&gt;export&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;VERSION&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;1.3.0
curl -sSL https://github.com/streamthoughts/kafka-connect-file-pulse/releases/download/v&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;/streamthoughts-kafka-connect-file-pulse-&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Extract it into one of the directories that is listed on the &lt;code&gt;plugin.path&lt;/code&gt; worker configuration property.&lt;/p&gt;
&lt;p&gt;You can also use the Confluent Hub CLI for installing it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ confluent-hub install --no-prompt streamthoughts-kafka-connect-file-pulse-&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Important&lt;/h4&gt;
When you run Connect workers in &lt;strong&gt;distributed mode&lt;/strong&gt;, the connector-plugin must be installed &lt;strong&gt;on each of machines&lt;/strong&gt; running Kafka Connect.
&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Installation</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/installation/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/installation/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Connect FilePulse&lt;/strong&gt; can be installed either from &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/releases&#34;&gt;GitHub Releases Page&lt;/a&gt; or from &lt;a href=&#34;https://www.confluent.io/hub/streamthoughts/kafka-connect-file-pulse&#34;&gt;Confluent Hub&lt;/a&gt;.&lt;/p&gt;


&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Caution&lt;/h4&gt;
You should note that the connector downloaded from Confluent Hub may not reflect the latest available version.
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Confluent Hub CLI installation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use the &lt;a href=&#34;https://docs.confluent.io/current/confluent-hub/client.html&#34;&gt;Confluent Hub client&lt;/a&gt; to install this connector with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;confluent-hub install streamthoughts/kafka-connect-file-pulse:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Download Installation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Download the distribution ZIP file for the latest available version.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example :&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#204a87&#34;&gt;export&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;VERSION&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;1.3.0
curl -sSL https://github.com/streamthoughts/kafka-connect-file-pulse/releases/download/v&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;/streamthoughts-kafka-connect-file-pulse-&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Extract it into one of the directories that is listed on the &lt;code&gt;plugin.path&lt;/code&gt; worker configuration property.&lt;/p&gt;
&lt;p&gt;You can also use the Confluent Hub CLI for installing it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ confluent-hub install --no-prompt streamthoughts-kafka-connect-file-pulse-&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Important&lt;/h4&gt;
When you run Connect workers in &lt;strong&gt;distributed mode&lt;/strong&gt;, the connector-plugin must be installed &lt;strong&gt;on each of machines&lt;/strong&gt; running Kafka Connect.
&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Installation</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/installation/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/installation/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Connect FilePulse&lt;/strong&gt; can be installed either from &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/releases&#34;&gt;GitHub Releases Page&lt;/a&gt; or from &lt;a href=&#34;https://www.confluent.io/hub/streamthoughts/kafka-connect-file-pulse&#34;&gt;Confluent Hub&lt;/a&gt;.&lt;/p&gt;


&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Caution&lt;/h4&gt;
You should note that the connector downloaded from Confluent Hub may not reflect the latest available version.
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Confluent Hub CLI installation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use the &lt;a href=&#34;https://docs.confluent.io/current/confluent-hub/client.html&#34;&gt;Confluent Hub client&lt;/a&gt; to install this connector with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;confluent-hub install streamthoughts/kafka-connect-file-pulse:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Download Installation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Download the distribution ZIP file for the latest available version.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example :&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#204a87&#34;&gt;export&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;VERSION&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;1.3.0
curl -sSL https://github.com/streamthoughts/kafka-connect-file-pulse/releases/download/v&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;/streamthoughts-kafka-connect-file-pulse-&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Extract it into one of the directories that is listed on the &lt;code&gt;plugin.path&lt;/code&gt; worker configuration property.&lt;/p&gt;
&lt;p&gt;You can also use the Confluent Hub CLI for installing it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ confluent-hub install --no-prompt streamthoughts-kafka-connect-file-pulse-&lt;span style=&#34;color:#000&#34;&gt;$VERSION&lt;/span&gt;.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Important&lt;/h4&gt;
When you run Connect workers in &lt;strong&gt;distributed mode&lt;/strong&gt;, the connector-plugin must be installed &lt;strong&gt;on each of machines&lt;/strong&gt; running Kafka Connect.
&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Getting the code</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/project-info/getting_the_code/</link>
      <pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/project-info/getting_the_code/</guid>
      <description>
        
        
        &lt;h2 id=&#34;prerequisites-for-building-connect-file-pulse&#34;&gt;Prerequisites for building Connect File Pulse&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Git&lt;/li&gt;
&lt;li&gt;Maven (we recommend version 3.5.3)&lt;/li&gt;
&lt;li&gt;Java 11&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;building-connect-file-pulse&#34;&gt;Building Connect File Pulse&lt;/h2&gt;
&lt;p&gt;The code of Connect File Pulse is kept in GitHub. You can check it out like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ git clone https://github.com/streamthoughts/kafka-connect-file-pulse.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The project uses Maven, you can build it like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ &lt;span style=&#34;color:#204a87&#34;&gt;cd&lt;/span&gt; kafka-connect-file-pulse
$ mvn clean package -DskipTests
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Confluent Hub&lt;/h4&gt;
Connect File Pulse is packaged into an archive
file compatible with &lt;a href=&#34;https://docs.confluent.io/current/connect/managing/confluent-hub/client.html&#34;&gt;confluent-hub client&lt;/a&gt;.
&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Getting the code</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/project-info/getting_the_code/</link>
      <pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/project-info/getting_the_code/</guid>
      <description>
        
        
        &lt;h2 id=&#34;prerequisites-for-building-connect-file-pulse&#34;&gt;Prerequisites for building Connect File Pulse&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Git&lt;/li&gt;
&lt;li&gt;Maven (we recommend version 3.5.3)&lt;/li&gt;
&lt;li&gt;Java 11&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;building-connect-file-pulse&#34;&gt;Building Connect File Pulse&lt;/h2&gt;
&lt;p&gt;The code of Connect File Pulse is kept in GitHub. You can check it out like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ git clone https://github.com/streamthoughts/kafka-connect-file-pulse.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The project uses Maven, you can build it like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ &lt;span style=&#34;color:#204a87&#34;&gt;cd&lt;/span&gt; kafka-connect-file-pulse
$ mvn clean package -DskipTests
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Confluent Hub&lt;/h4&gt;
Connect File Pulse is packaged into an archive
file compatible with &lt;a href=&#34;https://docs.confluent.io/current/connect/managing/confluent-hub/client.html&#34;&gt;confluent-hub client&lt;/a&gt;.
&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Getting the code</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/project-info/getting_the_code/</link>
      <pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/project-info/getting_the_code/</guid>
      <description>
        
        
        &lt;h2 id=&#34;prerequisites-for-building-connect-file-pulse&#34;&gt;Prerequisites for building Connect File Pulse&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Git&lt;/li&gt;
&lt;li&gt;Maven (we recommend version 3.5.3)&lt;/li&gt;
&lt;li&gt;Java 11&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;building-connect-file-pulse&#34;&gt;Building Connect File Pulse&lt;/h2&gt;
&lt;p&gt;The code of Connect File Pulse is kept in GitHub. You can check it out like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ git clone https://github.com/streamthoughts/kafka-connect-file-pulse.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The project uses Maven, you can build it like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ &lt;span style=&#34;color:#204a87&#34;&gt;cd&lt;/span&gt; kafka-connect-file-pulse
$ mvn clean package -DskipTests
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Confluent Hub&lt;/h4&gt;
Connect File Pulse is packaged into an archive
file compatible with &lt;a href=&#34;https://docs.confluent.io/current/connect/managing/confluent-hub/client.html&#34;&gt;confluent-hub client&lt;/a&gt;.
&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Getting the code</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/project-info/getting_the_code/</link>
      <pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/project-info/getting_the_code/</guid>
      <description>
        
        
        &lt;h2 id=&#34;prerequisites-for-building-connect-file-pulse&#34;&gt;Prerequisites for building Connect File Pulse&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Git&lt;/li&gt;
&lt;li&gt;Maven (we recommend version 3.5.3)&lt;/li&gt;
&lt;li&gt;Java 11&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;building-connect-file-pulse&#34;&gt;Building Connect File Pulse&lt;/h2&gt;
&lt;p&gt;The code of Connect File Pulse is kept in GitHub. You can check it out like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ git clone https://github.com/streamthoughts/kafka-connect-file-pulse.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The project uses Maven, you can build it like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ &lt;span style=&#34;color:#204a87&#34;&gt;cd&lt;/span&gt; kafka-connect-file-pulse
$ mvn clean package -DskipTests
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Confluent Hub&lt;/h4&gt;
Connect File Pulse is packaged into an archive
file compatible with &lt;a href=&#34;https://docs.confluent.io/current/connect/managing/confluent-hub/client.html&#34;&gt;confluent-hub client&lt;/a&gt;.
&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Getting the code</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/project-info/getting_the_code/</link>
      <pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/project-info/getting_the_code/</guid>
      <description>
        
        
        &lt;h2 id=&#34;prerequisites-for-building-connect-file-pulse&#34;&gt;Prerequisites for building Connect File Pulse&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Git&lt;/li&gt;
&lt;li&gt;Maven (we recommend version 3.5.3)&lt;/li&gt;
&lt;li&gt;Java 11&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;building-connect-file-pulse&#34;&gt;Building Connect File Pulse&lt;/h2&gt;
&lt;p&gt;The code of Connect File Pulse is kept in GitHub. You can check it out like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ git clone https://github.com/streamthoughts/kafka-connect-file-pulse.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The project uses Maven, you can build it like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ &lt;span style=&#34;color:#204a87&#34;&gt;cd&lt;/span&gt; kafka-connect-file-pulse
$ mvn clean package -DskipTests
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Confluent Hub&lt;/h4&gt;
Connect File Pulse is packaged into an archive
file compatible with &lt;a href=&#34;https://docs.confluent.io/current/connect/managing/confluent-hub/client.html&#34;&gt;confluent-hub client&lt;/a&gt;.
&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Getting the code</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/project-info/getting_the_code/</link>
      <pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/project-info/getting_the_code/</guid>
      <description>
        
        
        &lt;h2 id=&#34;prerequisites-for-building-connect-file-pulse&#34;&gt;Prerequisites for building Connect File Pulse&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Git&lt;/li&gt;
&lt;li&gt;Maven (we recommend version 3.5.3)&lt;/li&gt;
&lt;li&gt;Java 11&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;building-connect-file-pulse&#34;&gt;Building Connect File Pulse&lt;/h2&gt;
&lt;p&gt;The code of Connect File Pulse is kept in GitHub. You can check it out like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ git clone https://github.com/streamthoughts/kafka-connect-file-pulse.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The project uses Maven, you can build it like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ &lt;span style=&#34;color:#204a87&#34;&gt;cd&lt;/span&gt; kafka-connect-file-pulse
$ mvn clean package -DskipTests
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Confluent Hub&lt;/h4&gt;
Connect File Pulse is packaged into an archive
file compatible with &lt;a href=&#34;https://docs.confluent.io/current/connect/managing/confluent-hub/client.html&#34;&gt;confluent-hub client&lt;/a&gt;.
&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Configuration</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/configuration/</link>
      <pubDate>Wed, 12 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/configuration/</guid>
      <description>
        
        
        &lt;h2 id=&#34;commons-configuration&#34;&gt;Commons configuration&lt;/h2&gt;
&lt;p&gt;Whatever the kind of files you are processing a connector should always be configured with the below properties.
These configurations are described in detail in subsequent chapters.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Common Kafka Connect properties&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The default output topic to write&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tasks.max&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The maximum number of tasks that should be created for this connector.&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Properties for listing and cleaning object files (&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/file-listing/&#34;&gt;FileSystemListing&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.listing.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Class which is used to list eligible files from the scanned file system.&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.scanner.local.LocalFSDirectoryWalker&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.listing.filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Filters use to list eligible input files&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.listing.interval.ms&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Time interval (in milliseconds) at wish to scan input directory&lt;/td&gt;
&lt;td&gt;long&lt;/td&gt;
&lt;td&gt;&lt;em&gt;10000&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.cleanup.policy.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used to cleanup files&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;max.scheduled.files&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Maximum number of files that can be schedules to tasks.&lt;/td&gt;
&lt;td&gt;long&lt;/td&gt;
&lt;td&gt;&lt;em&gt;1000&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Properties for transforming object file record(&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters-chain-definition/&#34;&gt;Filters Chain Definition&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;List of filters aliases to apply on each data (order is important)&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Properties for reading object file record(&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/file-readers/&#34;&gt;FileReaders&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tasks.reader.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used by tasks to read input files&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.reader.RowFileReader&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Properties for uniquely identifying object files and records (&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/file-readers/&#34;&gt;FileReaders&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;offset.policy.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Class which is used to determine the source partition and offset that uniquely identify a input record&lt;/td&gt;
&lt;td&gt;&lt;code&gt;class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.offset.DefaultSourceOffsetPolicy&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Properties for synchronizing Connector and Tasks&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tasks.file.status.storage.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The FileObjectStateBackingStore class to be used for storing status state of file objects.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;io.streamthoughts.kafka.connect.filepulse.state.KafkaFileObjectStateBackingStore&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Available implementations are :&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;io.streamthoughts.kafka.connect.filepulse.state.InMemoryStateBackingStore&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;io.streamthoughts.kafka.connect.filepulse.state.KafkaFileObjectStateBackingStore&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Properties for configuring the &lt;code&gt;KafkaFileObjectStateBackingStore&lt;/code&gt; class&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tasks.file.status.storage.topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of the internal topic used by tasks and connector to report and monitor file progression.&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;connect-file-pulse-status&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tasks.file.status.storage.bootstrap.servers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A list of host/port pairs uses by the reporter for establishing the initial connection to the Kafka cluster.&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tasks.file.status.storage.topic.partitions&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The number of partitions to be used for the status storage topic.&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;LOW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tasks.file.status.storage.topic.replication.factor&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The replication factor to be used for the status storage topic.&lt;/td&gt;
&lt;td&gt;float&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;LOW&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;override-internal-consumerproducer-configuration&#34;&gt;Override Internal Consumer/Producer Configuration&lt;/h3&gt;
&lt;p&gt;To override the default configuration for the internal consumer and producer clients used for reporting file status, you can use one of the following override prefixes :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tasks.file.status.storage.consumer.&amp;lt;consumer_property&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tasks.file.status.storage.producer.&amp;lt;producer_property&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Some configuration examples are available &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/tree/master/examples&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Configuration</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/configuration/</link>
      <pubDate>Wed, 12 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/configuration/</guid>
      <description>
        
        
        &lt;h2 id=&#34;commons-configuration&#34;&gt;Commons configuration&lt;/h2&gt;
&lt;p&gt;Whatever the kind of files you are processing a connector should always be configured with the below properties.
These configurations are described in detail in subsequent chapters.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Common Kafka Connect properties&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The default output topic to write&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tasks.max&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The maximum number of tasks that should be created for this connector.&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Properties for listing and cleaning object files (&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/file-listing/&#34;&gt;FileSystemListing&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.listing.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Class which is used to list eligible files from the scanned file system.&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.listing.filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Filters use to list eligible input files&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.listing.interval.ms&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Time interval (in milliseconds) at wish to scan input directory&lt;/td&gt;
&lt;td&gt;long&lt;/td&gt;
&lt;td&gt;&lt;em&gt;10000&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.cleanup.policy.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used to cleanup files&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;max.scheduled.files&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Maximum number of files that can be schedules to tasks.&lt;/td&gt;
&lt;td&gt;long&lt;/td&gt;
&lt;td&gt;&lt;em&gt;1000&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Properties for transforming object file record(&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters-chain-definition/&#34;&gt;Filters Chain Definition&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;List of filters aliases to apply on each data (order is important)&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Properties for reading object file record(&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/file-readers/&#34;&gt;FileReaders&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tasks.reader.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used by tasks to read input files&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Properties for uniquely identifying object files and records (&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/file-readers/&#34;&gt;FileReaders&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;offset.policy.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Class which is used to determine the source partition and offset that uniquely identify a input record&lt;/td&gt;
&lt;td&gt;&lt;code&gt;class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.offset.DefaultSourceOffsetPolicy&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Properties for synchronizing Connector and Tasks&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tasks.file.status.storage.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The FileObjectStateBackingStore class to be used for storing status state of file objects.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;io.streamthoughts.kafka.connect.filepulse.state.KafkaFileObjectStateBackingStore&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Available implementations are :&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;io.streamthoughts.kafka.connect.filepulse.state.InMemoryFileObjectStateBackingStore&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;io.streamthoughts.kafka.connect.filepulse.state.KafkaFileObjectStateBackingStore&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Limitation&lt;/h4&gt;
The &lt;code&gt;InMemoryFileObjectStateBackingStore&lt;/code&gt; implement is not fault-tolerant and should be only when using Kafka Connect in standalone mode or a single worker.
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Properties for configuring the &lt;code&gt;KafkaFileObjectStateBackingStore&lt;/code&gt; class&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tasks.file.status.storage.topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of the internal topic used by tasks and connector to report and monitor file progression.&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;connect-file-pulse-status&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tasks.file.status.storage.bootstrap.servers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A list of host/port pairs uses by the reporter for establishing the initial connection to the Kafka cluster.&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tasks.file.status.storage.topic.partitions&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The number of partitions to be used for the status storage topic.&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;LOW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tasks.file.status.storage.topic.replication.factor&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The replication factor to be used for the status storage topic.&lt;/td&gt;
&lt;td&gt;float&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;LOW&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Some configuration examples are available &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/tree/master/examples&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Basic Configuration</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/developer-guide/configuration/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/developer-guide/configuration/</guid>
      <description>
        
        
        &lt;h2 id=&#34;commons-configuration&#34;&gt;Commons configuration&lt;/h2&gt;
&lt;p&gt;Whatever the kind of files you are processing a connector should always be configured with the below properties.
Those configuration are described in detail in subsequent chapters.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scanner.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used to scan file system&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.scanner.local.LocalFSDirectoryWalker&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.cleanup.policy.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used to cleanup files&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.directory.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The input directory to scan&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.interval.ms&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Time interval (in milliseconds) at wish to scan input directory&lt;/td&gt;
&lt;td&gt;long&lt;/td&gt;
&lt;td&gt;&lt;em&gt;10000&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Filters use to list eligible input files&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;List of filters aliases to apply on each data (order is important)&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;internal.kafka.reporter.topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of the internal topic used by tasks and connector to report and monitor file progression.&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;connect-file-pulse-status&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;internal.kafka.reporter.bootstrap.servers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A list of host/port pairs uses by the reporter for establishing the initial connection to the Kafka cluster.&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;task.reader.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used by tasks to read input files&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.reader.RowFileReader&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;offset.strategy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A separated list of attributes, using &lt;code&gt;+&lt;/code&gt;  as a character separator, to be used for uniquely identifying an input file; must be one of [&lt;code&gt;name&lt;/code&gt;, &lt;code&gt;path&lt;/code&gt;, &lt;code&gt;lastModified&lt;/code&gt;, &lt;code&gt;inode&lt;/code&gt;, &lt;code&gt;hash&lt;/code&gt;] (e.g: &lt;code&gt;name+hash&lt;/code&gt;). Note that order doesn&#39;t matter.&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;path+name&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The default output topic to write&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;prior-to-connect-filepulse-13x-deprecated&#34;&gt;Prior to Connect FilePulse 1.3.x (deprecated)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;internal.kafka.reporter.id&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The reporter identifier to be used by tasks and connector to report and monitor file progression (default null). This property must only be set for users that have run a connector in version prior to 1.3.x to ensure backward-compatibility (when set, must be unique for each connect instance).&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Basic Configuration</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/developer-guide/configuration/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/developer-guide/configuration/</guid>
      <description>
        
        
        &lt;h2 id=&#34;commons-configuration&#34;&gt;Commons configuration&lt;/h2&gt;
&lt;p&gt;Whatever the kind of files you are processing a connector should always be configured with the below properties.
Those configuration are described in detail in subsequent chapters.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scanner.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used to scan file system&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.scanner.local.LocalFSDirectoryWalker&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.cleanup.policy.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used to cleanup files&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.directory.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The input directory to scan&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.interval.ms&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Time interval (in milliseconds) at wish to scan input directory&lt;/td&gt;
&lt;td&gt;long&lt;/td&gt;
&lt;td&gt;&lt;em&gt;10000&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Filters use to list eligible input files&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;List of filters aliases to apply on each data (order is important)&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;internal.kafka.reporter.topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of the internal topic used by tasks and connector to report and monitor file progression.&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;connect-file-pulse-status&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;internal.kafka.reporter.bootstrap.servers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A list of host/port pairs uses by the reporter for establishing the initial connection to the Kafka cluster.&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;task.reader.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used by tasks to read input files&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.reader.RowFileReader&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;offset.strategy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A separated list of attributes, using &lt;code&gt;+&lt;/code&gt;  as a character separator, to be used for uniquely identifying an input file; must be one of [&lt;code&gt;name&lt;/code&gt;, &lt;code&gt;path&lt;/code&gt;, &lt;code&gt;lastModified&lt;/code&gt;, &lt;code&gt;inode&lt;/code&gt;, &lt;code&gt;hash&lt;/code&gt;] (e.g: &lt;code&gt;name+hash&lt;/code&gt;). Note that order doesn&#39;t matter.&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;path+name&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The default output topic to write&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;prior-to-connect-filepulse-13x-deprecated&#34;&gt;Prior to Connect FilePulse 1.3.x (deprecated)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;internal.kafka.reporter.id&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The reporter identifier to be used by tasks and connector to report and monitor file progression (default null). This property must only be set for users that have run a connector in version prior to 1.3.x to ensure backward-compatibility (when set, must be unique for each connect instance).&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Basic Configuration</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/developer-guide/configuration/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/developer-guide/configuration/</guid>
      <description>
        
        
        &lt;h2 id=&#34;commons-configuration&#34;&gt;Commons configuration&lt;/h2&gt;
&lt;p&gt;Whatever the kind of files you are processing a connector should always be configured with the below properties.
Those configuration are described in detail in subsequent chapters.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scanner.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used to scan file system&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.scanner.local.LocalFSDirectoryWalker&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.cleanup.policy.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used to cleanup files&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.directory.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The input directory to scan&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.interval.ms&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Time interval (in milliseconds) at wish to scan input directory&lt;/td&gt;
&lt;td&gt;long&lt;/td&gt;
&lt;td&gt;&lt;em&gt;10000&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Filters use to list eligible input files&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;List of filters aliases to apply on each data (order is important)&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;internal.kafka.reporter.topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of the internal topic used by tasks and connector to report and monitor file progression.&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;connect-file-pulse-status&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;internal.kafka.reporter.bootstrap.servers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A list of host/port pairs uses by the reporter for establishing the initial connection to the Kafka cluster.&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;task.reader.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used by tasks to read input files&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.reader.RowFileReader&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;offset.strategy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A separated list of attributes, using &lt;code&gt;+&lt;/code&gt;  as a character separator, to be used for uniquely identifying an input file; must be one of [&lt;code&gt;name&lt;/code&gt;, &lt;code&gt;path&lt;/code&gt;, &lt;code&gt;lastModified&lt;/code&gt;, &lt;code&gt;inode&lt;/code&gt;, &lt;code&gt;hash&lt;/code&gt;] (e.g: &lt;code&gt;name+hash&lt;/code&gt;). Note that order doesn&#39;t matter.&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;path+name&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The default output topic to write&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;prior-to-connect-filepulse-13x-deprecated&#34;&gt;Prior to Connect FilePulse 1.3.x (deprecated)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;internal.kafka.reporter.id&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The reporter identifier to be used by tasks and connector to report and monitor file progression (default null). This property must only be set for users that have run a connector in version prior to 1.3.x to ensure backward-compatibility (when set, must be unique for each connect instance).&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Basic Configuration</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/developer-guide/configuration/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/developer-guide/configuration/</guid>
      <description>
        
        
        &lt;h2 id=&#34;commons-configuration&#34;&gt;Commons configuration&lt;/h2&gt;
&lt;p&gt;Whatever the kind of files you are processing a connector should always be configured with the below properties.
Those configuration are described in detail in subsequent chapters.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scanner.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used to scan file system&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.scanner.local.LocalFSDirectoryWalker&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.cleanup.policy.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used to cleanup files&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.directory.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The input directory to scan&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.interval.ms&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Time interval (in milliseconds) at wish to scan input directory&lt;/td&gt;
&lt;td&gt;long&lt;/td&gt;
&lt;td&gt;&lt;em&gt;10000&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Filters use to list eligible input files&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;List of filters aliases to apply on each data (order is important)&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;internal.kafka.reporter.topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of the internal topic used by tasks and connector to report and monitor file progression.&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;connect-file-pulse-status&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;internal.kafka.reporter.bootstrap.servers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A list of host/port pairs uses by the reporter for establishing the initial connection to the Kafka cluster.&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;task.reader.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The fully qualified name of the class which is used by tasks to read input files&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.reader.RowFileReader&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;offset.strategy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The strategy to use for building source offset from an input file; must be one of [name, path, name+hash]&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;name+hash&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The default output topic to write&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;prior-to-connect-filepulse-13x-deprecated&#34;&gt;Prior to Connect FilePulse 1.3.x (deprecated)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;internal.kafka.reporter.id&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The reporter identifier to be used by tasks and connector to report and monitor file progression (default null). This property must only be set for users that have run a connector in version prior to 1.3.x to ensure backward-compatibility (when set, must be unique for each connect instance).&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: FileSystem Listing</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/file-system-listing/</link>
      <pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/file-system-listing/</guid>
      <description>
        
        
        &lt;p&gt;The &lt;code&gt;FilePulseSourceConnector&lt;/code&gt; periodically lists object files that may be streamed into Kafka using the &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-api/src/main/java/io/streamthoughts/kafka/connect/filepulse/fs/FileSystemListing.java&#34;&gt;FileSystemListing&lt;/a&gt;&lt;br&gt;
configured in the connector&#39;s configuration.&lt;/p&gt;
&lt;h2 id=&#34;supported-filesystems&#34;&gt;Supported Filesystems&lt;/h2&gt;
&lt;p&gt;Currently, Kafka Connect FilePulse supports the following implementations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;AmazonS3FileSystemListing&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AzureBlobStorageFileSystemListing&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GcsFileSystemListing&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LocalFSDirectoryListing&lt;/code&gt; (default)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;local-filesystem-default&#34;&gt;Local Filesystem (default)&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;LocalFSDirectoryListing&lt;/code&gt; class can be used for listing files that exist in a local filesystem directory.&lt;/p&gt;
&lt;h4 id=&#34;how-to-use-it-&#34;&gt;How to use it ?&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;fs.listing.class=io.streamthoughts.kafka.connect.filepulse.fs.LocalFSDirectoryListing&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;configuration&#34;&gt;Configuration&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.listing.directory.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The input directory to scan&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.listing.recursive.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Flag indicating whether local directory should be recursively scanned&lt;/td&gt;
&lt;td&gt;&lt;code&gt;boolean&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;supported-file-types&#34;&gt;Supported File types&lt;/h4&gt;
&lt;p&gt;The &lt;code&gt;LocalFSDirectoryListing&lt;/code&gt; will try to detect if a file needs to be decompressed by probing its content type or its extension (javadoc : &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/nio/file/Files.html#probeContentType-java.nio.file.Path&#34;&gt;Files#probeContentType&lt;/a&gt;)
Supported content-types are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GZIP&lt;/strong&gt; : &lt;code&gt;application/x-gzip&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TAR&lt;/strong&gt; : &lt;code&gt;application/x-tar&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ZIP&lt;/strong&gt; : &lt;code&gt;application/x-zip-compressed&lt;/code&gt; or &lt;code&gt;application/zip&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;amazon-s3&#34;&gt;Amazon S3&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;AmazonS3FileSystemListing&lt;/code&gt; class can be used for listing objects that exist in a specific Amazon S3 bucket.&lt;/p&gt;
&lt;h4 id=&#34;how-to-use-it-1&#34;&gt;How to use it ?&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;fs.listing.class=io.streamthoughts.kafka.connect.filepulse.fs.AmazonS3FileSystemListing&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;configuration1&#34;&gt;Configuration&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.access.key.id&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;AWS Access Key ID AWS&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.secret.access.key&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;AWS Secret Access Key&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.secret.session.token&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;AWS Secret Session Token&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.s3.region&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The AWS S3 Region, e.g. us-east-1&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Regions.DEFAULT_REGION.getName()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.s3.path.style.access.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Configures the client to use path-style access for all requests.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.s3.bucket.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The name of the Amazon S3 bucket.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.s3.bucket.prefix&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The prefix to be used for restricting the listing of the objects in the bucket&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.credentials.provider.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The AWSCredentialsProvider to use if no access key id and secret access key is configured.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;com.amazonaws.auth.EnvironmentVariableCredentialsProvider&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;LOW&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;google-cloud-storage&#34;&gt;Google Cloud Storage&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;GcsFileSystemListing&lt;/code&gt; class can be used for listing objects that exist in a specific Google Cloud Storage bucket.&lt;/p&gt;
&lt;h4 id=&#34;how-to-use-it-2&#34;&gt;How to use it ?&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;fs.listing.class=io.streamthoughts.kafka.connect.filepulse.fs.GcsFileSystemListing&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;configuration2&#34;&gt;Configuration&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;gcs.credentials.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The path to GCP credentials file. Cannot be set when &lt;code&gt;GCS_CREDENTIALS_JSON_CONFIG&lt;/code&gt; is provided. If no credentials is specified the client library will look for credentials via the environment variable &lt;code&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/code&gt;.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;gcs.credentials.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The GCP credentials as JSON string. Cannot be set when &lt;code&gt;GCS_CREDENTIALS_PATH_CONFIG&lt;/code&gt; is provided. If no credentials is specified the client library will look for credentials via the environment variable &lt;code&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/code&gt;.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;gcs.bucket.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The GCS bucket name to download the object files from.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;gcs.blobs.filter.prefix&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The prefix to be used for filtering blobs  whose names begin with it.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;azure-blob-storage&#34;&gt;Azure Blob Storage&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;AzureBlobStorageConfig&lt;/code&gt; class can be used for listing objects that exist in a specific Azure Storage Container.&lt;/p&gt;
&lt;h4 id=&#34;how-to-use-it-3&#34;&gt;How to use it ?&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;fs.listing.class=io.streamthoughts.kafka.connect.filepulse.fs.AzureBlobStorageConfig&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;configuration3&#34;&gt;Configuration&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;azure.storage.connection.string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Azure storage account connection string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;azure.storage.account.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The Azure storage account name.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;azure.storage.account.key&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The Azure storage account key.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;azure.storage.container.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The Azure storage container name.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;azure.storage.blob.prefix&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The prefix to be used for restricting the listing of the blobs in the container.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;filtering-input-files&#34;&gt;Filtering input files&lt;/h2&gt;
&lt;p&gt;You can configure one or more &lt;code&gt;FileFilter&lt;/code&gt; that will be used to determine if a file should be scheduled for processing or ignored.
All files that are filtered out are simply ignored and remain untouched on the file system until the next scan.
At the next scan, previously filtered files will be evaluated again to determine if they are now eligible for processing.&lt;/p&gt;
&lt;p&gt;FilePulse packs with the following built-in filters :&lt;/p&gt;
&lt;h3 id=&#34;ignorehiddenfilefilter&#34;&gt;IgnoreHiddenFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;IgnoreHiddenFileFilter&lt;/code&gt; can be used to filter hidden files from being read.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configuration example&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.listing.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.IgnoreHiddenFileListFilter
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Limitation&lt;/h4&gt;
This filter is only supported by the &lt;code&gt;LocalFSDirectoryListing&lt;/code&gt;.
&lt;/div&gt;

&lt;h3 id=&#34;lastmodifiedfilefilter&#34;&gt;LastModifiedFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;LastModifiedFileFilter&lt;/code&gt; can be used to filter files that have been modified to recently based on their last modified date property.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.listing.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.LastModifiedFileFilter
# The last modified time for a file can be accepted (default: 5000)
file.filter.minimum.age.ms=10000
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;regexfilefilter&#34;&gt;RegexFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;RegexFileFilter&lt;/code&gt; can be used to filter files that do not match the specified regex.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.listing.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.RegexFileListFilter
# The regex pattern used to matches input files
file.filter.regex.pattern=&amp;quot;\\.log$&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: FileSystem Listing</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/file-system-listing/</link>
      <pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/file-system-listing/</guid>
      <description>
        
        
        &lt;p&gt;The &lt;code&gt;FilePulseSourceConnector&lt;/code&gt; periodically lists object files that may be streamed into Kafka using the &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-api/src/main/java/io/streamthoughts/kafka/connect/filepulse/fs/FileSystemListing.java&#34;&gt;FileSystemListing&lt;/a&gt;&lt;br&gt;
configured in the connector&#39;s configuration.&lt;/p&gt;
&lt;h2 id=&#34;supported-filesystems&#34;&gt;Supported Filesystems&lt;/h2&gt;
&lt;p&gt;Currently, Kafka Connect FilePulse supports the following implementations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;AmazonS3FileSystemListing&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AzureBlobStorageFileSystemListing&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GcsFileSystemListing&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LocalFSDirectoryListing&lt;/code&gt; (default)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;local-filesystem-default&#34;&gt;Local Filesystem (default)&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;LocalFSDirectoryListing&lt;/code&gt; class can be used for listing files that exist in a local filesystem directory.&lt;/p&gt;
&lt;h4 id=&#34;how-to-use-it-&#34;&gt;How to use it ?&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;fs.listing.class=io.streamthoughts.kafka.connect.filepulse.fs.LocalFSDirectoryListing&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;configuration&#34;&gt;Configuration&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.listing.directory.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The input directory to scan&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.listing.recursive.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Flag indicating whether local directory should be recursively scanned&lt;/td&gt;
&lt;td&gt;&lt;code&gt;boolean&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;supported-file-types&#34;&gt;Supported File types&lt;/h4&gt;
&lt;p&gt;The &lt;code&gt;LocalFSDirectoryListing&lt;/code&gt; will try to detect if a file needs to be decompressed by probing its content type or its extension (javadoc : &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/nio/file/Files.html#probeContentType-java.nio.file.Path&#34;&gt;Files#probeContentType&lt;/a&gt;)
Supported content-types are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GZIP&lt;/strong&gt; : &lt;code&gt;application/x-gzip&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TAR&lt;/strong&gt; : &lt;code&gt;application/x-tar&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ZIP&lt;/strong&gt; : &lt;code&gt;application/x-zip-compressed&lt;/code&gt; or &lt;code&gt;application/zip&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;amazon-s3&#34;&gt;Amazon S3&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;AmazonS3FileSystemListing&lt;/code&gt; class can be used for listing objects that exist in a specific Amazon S3 bucket.&lt;/p&gt;
&lt;h4 id=&#34;how-to-use-it-1&#34;&gt;How to use it ?&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;fs.listing.class=io.streamthoughts.kafka.connect.filepulse.fs.AmazonS3FileSystemListing&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;configuration1&#34;&gt;Configuration&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.access.key.id&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;AWS Access Key ID AWS&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.secret.access.key&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;AWS Secret Access Key&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.secret.session.token&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;AWS Secret Session Token&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.s3.region&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The AWS S3 Region, e.g. us-east-1&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Regions.DEFAULT_REGION.getName()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.s3.path.style.access.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Configures the client to use path-style access for all requests.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.s3.bucket.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The name of the Amazon S3 bucket.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.s3.bucket.prefix&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The prefix to be used for restricting the listing of the objects in the bucket&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;aws.credentials.provider.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The AWSCredentialsProvider to use if no access key id and secret access key is configured.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;com.amazonaws.auth.EnvironmentVariableCredentialsProvider&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;LOW&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;google-cloud-storage&#34;&gt;Google Cloud Storage&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;GcsFileSystemListing&lt;/code&gt; class can be used for listing objects that exist in a specific Google Cloud Storage bucket.&lt;/p&gt;
&lt;h4 id=&#34;how-to-use-it-2&#34;&gt;How to use it ?&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;fs.listing.class=io.streamthoughts.kafka.connect.filepulse.fs.GcsFileSystemListing&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;configuration2&#34;&gt;Configuration&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;gcs.credentials.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The path to GCP credentials file. Cannot be set when &lt;code&gt;GCS_CREDENTIALS_JSON_CONFIG&lt;/code&gt; is provided. If no credentials is specified the client library will look for credentials via the environment variable &lt;code&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/code&gt;.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;gcs.credentials.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The GCP credentials as JSON string. Cannot be set when &lt;code&gt;GCS_CREDENTIALS_PATH_CONFIG&lt;/code&gt; is provided. If no credentials is specified the client library will look for credentials via the environment variable &lt;code&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/code&gt;.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;gcs.bucket.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The GCS bucket name to download the object files from.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;gcs.blobs.filter.prefix&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The prefix to be used for filtering blobs  whose names begin with it.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;azure-blob-storage&#34;&gt;Azure Blob Storage&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;AzureBlobStorageConfig&lt;/code&gt; class can be used for listing objects that exist in a specific Azure Storage Container.&lt;/p&gt;
&lt;h4 id=&#34;how-to-use-it-3&#34;&gt;How to use it ?&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;fs.listing.class=io.streamthoughts.kafka.connect.filepulse.fs.AzureBlobStorageConfig&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;configuration3&#34;&gt;Configuration&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;azure.storage.connection.string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Azure storage account connection string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;azure.storage.account.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The Azure storage account name.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;azure.storage.account.key&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The Azure storage account key.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;azure.storage.container.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The Azure storage container name.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;azure.storage.blob.prefix&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The prefix to be used for restricting the listing of the blobs in the container.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;MEDIUM&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;filtering-input-files&#34;&gt;Filtering input files&lt;/h2&gt;
&lt;p&gt;You can configure one or more &lt;code&gt;FileFilter&lt;/code&gt; that will be used to determine if a file should be scheduled for processing or ignored.
All files that are filtered out are simply ignored and remain untouched on the file system until the next scan.
At the next scan, previously filtered files will be evaluated again to determine if they are now eligible for processing.&lt;/p&gt;
&lt;p&gt;FilePulse packs with the following built-in filters :&lt;/p&gt;
&lt;h3 id=&#34;ignorehiddenfilefilter&#34;&gt;IgnoreHiddenFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;IgnoreHiddenFileFilter&lt;/code&gt; can be used to filter hidden files from being read.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configuration example&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.listing.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.IgnoreHiddenFileListFilter
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Limitation&lt;/h4&gt;
This filter is only supported by the &lt;code&gt;LocalFSDirectoryListing&lt;/code&gt;.
&lt;/div&gt;

&lt;h3 id=&#34;lastmodifiedfilefilter&#34;&gt;LastModifiedFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;LastModifiedFileFilter&lt;/code&gt; can be used to filter files that have been modified to recently based on their last modified date property.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.listing.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.LastModifiedFileFilter
# The last modified time for a file can be accepted (default: 5000)
file.filter.minimum.age.ms=10000
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;regexfilefilter&#34;&gt;RegexFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;RegexFileFilter&lt;/code&gt; can be used to filter files that do not match the specified regex.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.listing.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.RegexFileListFilter
# The regex pattern used to matches input files
file.filter.regex.pattern=&amp;quot;\\.log$&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: Scanning Files</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/developer-guide/scanning-files/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/developer-guide/scanning-files/</guid>
      <description>
        
        
        &lt;p&gt;The connector must be configured with a specific &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/scanner/local/FSDirectoryWalker.java&#34;&gt;FSDirectoryWalker&lt;/a&gt;&lt;br&gt;
that will be responsible for scanning an input directory to find files eligible to be streamed in Kafka.&lt;/p&gt;
&lt;p&gt;The default &lt;code&gt;FSDirectoryWalker&lt;/code&gt; implementation is :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;io.streamthoughts.kafka.connect.filepulse.scanner.local.LocalFSDirectoryWalker&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;FilePulseSourceConnector&lt;/code&gt; periodically triggers a file system scan of the directory specified in the &lt;code&gt;input.directory.path&lt;/code&gt;
connector property. Scan is executed in a background-thread invoking the configured &lt;code&gt;FSDirectoryWalker&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;configuring-directory-scan-using-localfsdirectorywalker&#34;&gt;Configuring Directory Scan (using &lt;code&gt;LocalFSDirectoryWalker&lt;/code&gt;)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scanner.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The class used to scan file system&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.scanner.local.LocalFSDirectoryWalker&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.directory.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The input directory to scan&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.interval.ms&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Time interval in milliseconds at wish the input directory is scanned&lt;/td&gt;
&lt;td&gt;long&lt;/td&gt;
&lt;td&gt;&lt;em&gt;10000&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The comma-separated list of fully qualified class names of the filter-filters to be uses to list eligible input files&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.recursive.scan.enable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Boolean indicating whether local directory should be recursively scanned&lt;/td&gt;
&lt;td&gt;boolean&lt;/td&gt;
&lt;td&gt;&lt;em&gt;true&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;filtering-input-files&#34;&gt;Filtering input files&lt;/h2&gt;
&lt;p&gt;You can configure one or more &lt;code&gt;FileFilter&lt;/code&gt; that will be used to determine if a file should be scheduled for processing or ignored.
All files that are filtered out are simply ignored and remain untouched on the file system until the next scan.
At the next scan, previously filtered files will be evaluated again to determine if they are now eligible for processing.&lt;/p&gt;
&lt;p&gt;FilePulse packs with the following built-in filters :&lt;/p&gt;
&lt;h3 id=&#34;ignorehiddenfilefilter&#34;&gt;IgnoreHiddenFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;IgnoreHiddenFileFilter&lt;/code&gt; can be used to filter hidden files from being read.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configuration example&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.scan.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.IgnoreHiddenFileListFilter
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;lastmodifiedfilefilter&#34;&gt;LastModifiedFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;LastModifiedFileFilter&lt;/code&gt; can be used to filter files that have been modified to recently based on their last modified date property.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.scan.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.LastModifiedFileFilter
# The last modified time for a file can be accepted (default: 5000)
file.filter.minimum.age.ms=10000
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;regexfilefilter&#34;&gt;RegexFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;RegexFileFilter&lt;/code&gt; can be used to filter files that do not match the specified regex.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.scan.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.RegexFileListFilter
# The regex pattern used to matches input files
file.filter.regex.pattern=&amp;quot;\\.log$&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;supported-file-types&#34;&gt;Supported File types&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;LocalFSDirectoryWalker&lt;/code&gt; will try to detect if a file needs to be decompressed by probing its content type or its extension (javadoc : &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/nio/file/Files.html#probeContentType-java.nio.file.Path&#34;&gt;Files#probeContentType&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The connector supports the following content types :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GZIP&lt;/strong&gt; : &lt;code&gt;application/x-gzip&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TAR&lt;/strong&gt; : &lt;code&gt;application/x-tar&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ZIP&lt;/strong&gt; : &lt;code&gt;application/x-zip-compressed&lt;/code&gt; or &lt;code&gt;application/zip&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Scanning Files</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/developer-guide/scanning-files/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/developer-guide/scanning-files/</guid>
      <description>
        
        
        &lt;p&gt;The connector must be configured with a specific &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/scanner/local/FSDirectoryWalker.java&#34;&gt;FSDirectoryWalker&lt;/a&gt;&lt;br&gt;
that will be responsible for scanning an input directory to find files eligible to be streamed in Kafka.&lt;/p&gt;
&lt;p&gt;The default &lt;code&gt;FSDirectoryWalker&lt;/code&gt; implementation is :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;io.streamthoughts.kafka.connect.filepulse.scanner.local.LocalFSDirectoryWalker&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;FilePulseSourceConnector&lt;/code&gt; periodically triggers a file system scan of the directory specified in the &lt;code&gt;input.directory.path&lt;/code&gt;
connector property. Scan is executed in a background-thread invoking the configured &lt;code&gt;FSDirectoryWalker&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;configuring-directory-scan-using-localfsdirectorywalker&#34;&gt;Configuring Directory Scan (using &lt;code&gt;LocalFSDirectoryWalker&lt;/code&gt;)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scanner.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The class used to scan file system&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.scanner.local.LocalFSDirectoryWalker&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.directory.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The input directory to scan&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.interval.ms&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Time interval in milliseconds at wish the input directory is scanned&lt;/td&gt;
&lt;td&gt;long&lt;/td&gt;
&lt;td&gt;&lt;em&gt;10000&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The comma-separated list of fully qualified class names of the filter-filters to be uses to list eligible input files&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.recursive.scan.enable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Boolean indicating whether local directory should be recursively scanned&lt;/td&gt;
&lt;td&gt;boolean&lt;/td&gt;
&lt;td&gt;&lt;em&gt;true&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;filtering-input-files&#34;&gt;Filtering input files&lt;/h2&gt;
&lt;p&gt;You can configure one or more &lt;code&gt;FileFilter&lt;/code&gt; that will be used to determine if a file should be scheduled for processing or ignored.
All files that are filtered out are simply ignored and remain untouched on the file system until the next scan.
At the next scan, previously filtered files will be evaluated again to determine if they are now eligible for processing.&lt;/p&gt;
&lt;p&gt;FilePulse packs with the following built-in filters :&lt;/p&gt;
&lt;h3 id=&#34;ignorehiddenfilefilter&#34;&gt;IgnoreHiddenFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;IgnoreHiddenFileFilter&lt;/code&gt; can be used to filter hidden files from being read.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configuration example&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.scan.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.IgnoreHiddenFileListFilter
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;lastmodifiedfilefilter&#34;&gt;LastModifiedFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;LastModifiedFileFilter&lt;/code&gt; can be used to filter files that have been modified to recently based on their last modified date property.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.scan.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.LastModifiedFileFilter
# The last modified time for a file can be accepted (default: 5000)
file.filter.minimum.age.ms=10000
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;regexfilefilter&#34;&gt;RegexFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;RegexFileFilter&lt;/code&gt; can be used to filter files that do not match the specified regex.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.scan.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.RegexFileListFilter
# The regex pattern used to matches input files
file.filter.regex.pattern=&amp;quot;\\.log$&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;supported-file-types&#34;&gt;Supported File types&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;LocalFSDirectoryWalker&lt;/code&gt; will try to detect if a file needs to be decompressed by probing its content type or its extension (javadoc : &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/nio/file/Files.html#probeContentType-java.nio.file.Path&#34;&gt;Files#probeContentType&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The connector supports the following content types :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GZIP&lt;/strong&gt; : &lt;code&gt;application/x-gzip&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TAR&lt;/strong&gt; : &lt;code&gt;application/x-tar&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ZIP&lt;/strong&gt; : &lt;code&gt;application/x-zip-compressed&lt;/code&gt; or &lt;code&gt;application/zip&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Scanning Files</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/developer-guide/scanning-files/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/developer-guide/scanning-files/</guid>
      <description>
        
        
        &lt;p&gt;The connector must be configured with a specific &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/scanner/local/FSDirectoryWalker.java&#34;&gt;FSDirectoryWalker&lt;/a&gt;&lt;br&gt;
that will be responsible for scanning an input directory to find files eligible to be streamed in Kafka.&lt;/p&gt;
&lt;p&gt;The default &lt;code&gt;FSDirectoryWalker&lt;/code&gt; implementation is :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;io.streamthoughts.kafka.connect.filepulse.scanner.local.LocalFSDirectoryWalker&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;FilePulseSourceConnector&lt;/code&gt; periodically triggers a file system scan of the directory specified in the &lt;code&gt;input.directory.path&lt;/code&gt;
connector property. Scan is executed in a background-thread invoking the configured &lt;code&gt;FSDirectoryWalker&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;configuring-directory-scan-using-localfsdirectorywalker&#34;&gt;Configuring Directory Scan (using &lt;code&gt;LocalFSDirectoryWalker&lt;/code&gt;)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scanner.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The class used to scan file system&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.scanner.local.LocalFSDirectoryWalker&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.directory.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The input directory to scan&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.interval.ms&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Time interval in milliseconds at wish the input directory is scanned&lt;/td&gt;
&lt;td&gt;long&lt;/td&gt;
&lt;td&gt;&lt;em&gt;10000&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The comma-separated list of fully qualified class names of the filter-filters to be uses to list eligible input files&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.recursive.scan.enable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Boolean indicating whether local directory should be recursively scanned&lt;/td&gt;
&lt;td&gt;boolean&lt;/td&gt;
&lt;td&gt;&lt;em&gt;true&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;filtering-input-files&#34;&gt;Filtering input files&lt;/h2&gt;
&lt;p&gt;You can configure one or more &lt;code&gt;FileFilter&lt;/code&gt; that will be used to determine if a file should be scheduled for processing or ignored.
All files that are filtered out are simply ignored and remain untouched on the file system until the next scan.
At the next scan, previously filtered files will be evaluated again to determine if they are now eligible for processing.&lt;/p&gt;
&lt;p&gt;FilePulse packs with the following built-in filters :&lt;/p&gt;
&lt;h3 id=&#34;ignorehiddenfilefilter&#34;&gt;IgnoreHiddenFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;IgnoreHiddenFileFilter&lt;/code&gt; can be used to filter hidden files from being read.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configuration example&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.scan.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.IgnoreHiddenFileListFilter
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;lastmodifiedfilefilter&#34;&gt;LastModifiedFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;LastModifiedFileFilter&lt;/code&gt; can be used to filter files that have been modified to recently based on their last modified date property.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.scan.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.LastModifiedFileFilter
# The last modified time for a file can be accepted (default: 5000)
file.filter.minimum.age.ms=10000
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;regexfilefilter&#34;&gt;RegexFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;RegexFileFilter&lt;/code&gt; can be used to filter files that do not match the specified regex.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.scan.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.RegexFileListFilter
# The regex pattern used to matches input files
file.filter.regex.pattern=&amp;quot;\\.log$&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;supported-file-types&#34;&gt;Supported File types&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;LocalFSDirectoryWalker&lt;/code&gt; will try to detect if a file needs to be decompressed by probing its content type or its extension (javadoc : &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/nio/file/Files.html#probeContentType-java.nio.file.Path&#34;&gt;Files#probeContentType&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The connector supports the following content types :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GZIP&lt;/strong&gt; : &lt;code&gt;application/x-gzip&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TAR&lt;/strong&gt; : &lt;code&gt;application/x-tar&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ZIP&lt;/strong&gt; : &lt;code&gt;application/x-zip-compressed&lt;/code&gt; or &lt;code&gt;application/zip&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Scanning Files</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/developer-guide/scanning-files/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/developer-guide/scanning-files/</guid>
      <description>
        
        
        &lt;p&gt;The connector must be configured with a specific &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/scanner/local/FSDirectoryWalker.java&#34;&gt;FSDirectoryWalker&lt;/a&gt;&lt;br&gt;
that will be responsible for scanning an input directory to find files eligible to be streamed in Kafka.&lt;/p&gt;
&lt;p&gt;The default &lt;code&gt;FSDirectoryWalker&lt;/code&gt; implementation is :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;io.streamthoughts.kafka.connect.filepulse.scanner.local.LocalFSDirectoryWalker&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;FilePulseSourceConnector&lt;/code&gt; periodically triggers a file system scan of the directory specified in the &lt;code&gt;input.directory.path&lt;/code&gt;
connector property. Scan is executed in a background-thread invoking the configured &lt;code&gt;FSDirectoryWalker&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;configuring-directory-scan-using-localfsdirectorywalker&#34;&gt;Configuring Directory Scan (using &lt;code&gt;LocalFSDirectoryWalker&lt;/code&gt;)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scanner.class&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The class used to scan file system&lt;/td&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;&lt;em&gt;io.streamthoughts.kafka.connect.filepulse.scanner.local.LocalFSDirectoryWalker&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.directory.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The input directory to scan&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.interval.ms&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Time interval in milliseconds at wish the input directory is scanned&lt;/td&gt;
&lt;td&gt;long&lt;/td&gt;
&lt;td&gt;&lt;em&gt;10000&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.scan.filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The comma-separated list of fully qualified class names of the filter-filters to be uses to list eligible input files&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;em&gt;-&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fs.recursive.scan.enable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Boolean indicating whether local directory should be recursively scanned&lt;/td&gt;
&lt;td&gt;boolean&lt;/td&gt;
&lt;td&gt;&lt;em&gt;true&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;allow.tasks.reconfiguration.after.timeout.ms&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Specifies the timeout (in milliseconds) for the connector to allow tasks to be reconfigured when new files are detected, even if some tasks are still being processed&lt;/td&gt;
&lt;td&gt;long&lt;/td&gt;
&lt;td&gt;&lt;em&gt;Long.MAX_VALUE&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;filtering-input-files&#34;&gt;Filtering input files&lt;/h2&gt;
&lt;p&gt;You can configure one or more &lt;code&gt;FileFilter&lt;/code&gt; that will be used to determine if a file should be scheduled for processing or ignored.
All files that are filtered out are simply ignored and remain untouched on the file system until the next scan.
At the next scan, previously filtered files will be evaluated again to determine if they are now eligible for processing.&lt;/p&gt;
&lt;p&gt;FilePulse packs with the following built-in filters :&lt;/p&gt;
&lt;h3 id=&#34;ignorehiddenfilefilter&#34;&gt;IgnoreHiddenFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;IgnoreHiddenFileFilter&lt;/code&gt; can be used to filter hidden files from being read.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configuration example&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.scan.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.IgnoreHiddenFileListFilter
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;lastmodifiedfilefilter&#34;&gt;LastModifiedFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;LastModifiedFileFilter&lt;/code&gt; can be used to filter files that have been modified to recently based on their last modified date property.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.scan.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.LastModifiedFileFilter
# The last modified time for a file can be accepted (default: 5000)
file.filter.minimum.age.ms=10000
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;regexfilefilter&#34;&gt;RegexFileFilter&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;RegexFileFilter&lt;/code&gt; can be used to filter files that do not match the specified regex.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;fs.scan.filters=io.streamthoughts.kafka.connect.filepulse.scanner.local.filter.RegexFileListFilter
# The regex pattern used to matches input files
file.filter.regex.pattern=&amp;quot;\\.log$&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;supported-file-types&#34;&gt;Supported File types&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;LocalFSDirectoryWalker&lt;/code&gt; will try to detect if a file needs to be decompressed by probing its content type or its extension (javadoc : &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/nio/file/Files.html#probeContentType-java.nio.file.Path&#34;&gt;Files#probeContentType&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The connector supports the following content types :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GZIP&lt;/strong&gt; : &lt;code&gt;application/x-gzip&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TAR&lt;/strong&gt; : &lt;code&gt;application/x-tar&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ZIP&lt;/strong&gt; : &lt;code&gt;application/x-zip-compressed&lt;/code&gt; or &lt;code&gt;application/zip&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: File Readers</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/file-readers/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/file-readers/</guid>
      <description>
        
        
        &lt;p&gt;The &lt;code&gt;FilePulseSourceTask&lt;/code&gt; uses the &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-api/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/FileInputReader.java&#34;&gt;FileInputReader&lt;/a&gt;.
configured in the connector&#39;s configuration for reading object files (i.e., &lt;code&gt;tasks.reader.class&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Currently, Connect FilePulse provides the following &lt;code&gt;FileInputReader&lt;/code&gt; implementations :&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Amazon S3&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;package: &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.fs.reader&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;AmazonS3AvroFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AmazonS3BytesArrayInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AmazonS3RowFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AmazonS3XMLFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AmazonS3MetadataFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Azure Blob Storage&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;package: &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.fs.reader&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;AzureBlobStorageAvroFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AzureBlobStorageBytesArrayInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AzureBlobStorageRowFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AzureBlobStorageXMLFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AzureBlobStorageMetadataFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Google Cloud Storage&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;package: &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.fs.reader&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GcsAvroFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GcsBytesArrayInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GcsRowFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GcsXMLFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GcsMetadataFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Local Filesystem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;package: &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.fs.reader&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;LocalAvroFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LocalBytesArrayInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LocalRowFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LocalXMLFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LocalMetadataFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rowfileinputreader-default&#34;&gt;RowFileInputReader (default)&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;&amp;lt;PREFIX&amp;gt;RowFileInputReader&lt;/code&gt;s can be used to read files line by line.
This reader creates one record per row. It should be used for reading delimited text files, application log files, etc.&lt;/p&gt;
&lt;h3 id=&#34;configuration&#34;&gt;Configuration&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;file.encoding&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The text file encoding to use&lt;/td&gt;
&lt;td&gt;&lt;code&gt;String&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;UTF_8&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;buffer.initial.bytes.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The initial buffer size used to read input files.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;String&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;4096&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;min.read.records&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The minimum number of records to read from file before returning to task.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Integer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;skip.headers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The number of rows to be skipped in the beginning of file.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Integer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;skip.footers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The number of rows to be skipped at the end of file.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Integer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;read.max.wait.ms&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The maximum time to wait in milliseconds for more bytes after hitting end of file.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Long&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;xxxbytesarrayinputreader&#34;&gt;XxxBytesArrayInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;&amp;lt;PREFIX&amp;gt;BytesArrayInputReader&lt;/code&gt;s create a single byte array record from a source file.&lt;/p&gt;
&lt;h2 id=&#34;xxxavrofileinputreader&#34;&gt;XxxAvroFileInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;&amp;lt;PREFIX&amp;gt;AvroFileInputReader&lt;/code&gt;s can be used to read Avro files.&lt;/p&gt;
&lt;h2 id=&#34;xxxxmlfileinputreader&#34;&gt;XxxXMLFileInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;&amp;lt;PREFIX&amp;gt;XMLFileInputReader&lt;/code&gt;s can be used to read XML files.&lt;/p&gt;
&lt;h3 id=&#34;configuration1&#34;&gt;Configuration&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;xpath.expression&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The XPath expression used extract data from XML input files&lt;/td&gt;
&lt;td&gt;&lt;code&gt;String&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;xpath.result.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The expected result type for the XPath expression in [NODESET, STRING]&lt;/td&gt;
&lt;td&gt;&lt;code&gt;String&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;NODESET&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;force.array.on.fields&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The comma-separated list of fields for which an array-type must be forced&lt;/td&gt;
&lt;td&gt;&lt;code&gt;List&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;xxxmetadatafileinputreader&#34;&gt;XxxMetadataFileInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;FileInputMetadataReader&lt;/code&gt;s can be used to send a single record per file containing metadata, i.e.: &lt;code&gt;name&lt;/code&gt;, &lt;code&gt;path&lt;/code&gt;, &lt;code&gt;hash&lt;/code&gt;, &lt;code&gt;lastModified&lt;/code&gt;, &lt;code&gt;size&lt;/code&gt;, etc.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: File Readers</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/file-readers/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/file-readers/</guid>
      <description>
        
        
        &lt;p&gt;The &lt;code&gt;FilePulseSourceTask&lt;/code&gt; uses the &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-api/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/FileInputReader.java&#34;&gt;FileInputReader&lt;/a&gt;.
configured in the connector&#39;s configuration for reading object files (i.e., &lt;code&gt;tasks.reader.class&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Currently, Connect FilePulse provides the following &lt;code&gt;FileInputReader&lt;/code&gt; implementations :&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Amazon S3&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;package: &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.fs.reader&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;AmazonS3AvroFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AmazonS3BytesArrayInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AmazonS3RowFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AmazonS3XMLFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AmazonS3MetadataFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Azure Blob Storage&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;package: &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.fs.reader&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;AzureBlobStorageAvroFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AzureBlobStorageBytesArrayInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AzureBlobStorageRowFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AzureBlobStorageXMLFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AzureBlobStorageMetadataFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Google Cloud Storage&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;package: &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.fs.reader&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GcsAvroFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GcsBytesArrayInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GcsRowFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GcsXMLFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GcsMetadataFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Local Filesystem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;package: &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.fs.reader&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;LocalAvroFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LocalBytesArrayInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LocalRowFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LocalXMLFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LocalMetadataFileInputReader&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rowfileinputreader-default&#34;&gt;RowFileInputReader (default)&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;&amp;lt;PREFIX&amp;gt;RowFileInputReader&lt;/code&gt;s can be used to read files line by line.
This reader creates one record per row. It should be used for reading delimited text files, application log files, etc.&lt;/p&gt;
&lt;h3 id=&#34;configuration&#34;&gt;Configuration&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;file.encoding&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The text file encoding to use&lt;/td&gt;
&lt;td&gt;&lt;code&gt;String&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;UTF_8&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;buffer.initial.bytes.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The initial buffer size used to read input files.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;String&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;4096&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;min.read.records&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The minimum number of records to read from file before returning to task.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Integer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;skip.headers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The number of rows to be skipped in the beginning of file.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Integer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;skip.footers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The number of rows to be skipped at the end of file.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Integer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;read.max.wait.ms&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The maximum time to wait in milliseconds for more bytes after hitting end of file.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Long&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;xxxbytesarrayinputreader&#34;&gt;XxxBytesArrayInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;&amp;lt;PREFIX&amp;gt;BytesArrayInputReader&lt;/code&gt;s create a single byte array record from a source file.&lt;/p&gt;
&lt;h2 id=&#34;xxxavrofileinputreader&#34;&gt;XxxAvroFileInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;&amp;lt;PREFIX&amp;gt;AvroFileInputReader&lt;/code&gt;s can be used to read Avro files.&lt;/p&gt;
&lt;h2 id=&#34;xxxxmlfileinputreader&#34;&gt;XxxXMLFileInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;&amp;lt;PREFIX&amp;gt;XMLFileInputReader&lt;/code&gt;s can be used to read XML files.&lt;/p&gt;
&lt;h3 id=&#34;configuration1&#34;&gt;Configuration&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;xpath.expression&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The XPath expression used extract data from XML input files&lt;/td&gt;
&lt;td&gt;&lt;code&gt;String&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;xpath.result.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The expected result type for the XPath expression in [NODESET, STRING]&lt;/td&gt;
&lt;td&gt;&lt;code&gt;String&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;NODESET&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;force.array.on.fields&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The comma-separated list of fields for which an array-type must be forced&lt;/td&gt;
&lt;td&gt;&lt;code&gt;List&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;xxxmetadatafileinputreader&#34;&gt;XxxMetadataFileInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;FileInputMetadataReader&lt;/code&gt;s can be used to send a single record per file containing metadata, i.e.: &lt;code&gt;name&lt;/code&gt;, &lt;code&gt;path&lt;/code&gt;, &lt;code&gt;hash&lt;/code&gt;, &lt;code&gt;lastModified&lt;/code&gt;, &lt;code&gt;size&lt;/code&gt;, etc.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: File Readers</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/developer-guide/file-readers/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/developer-guide/file-readers/</guid>
      <description>
        
        
        &lt;p&gt;The connector can be configured with a specific &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-api/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/FileInputReader.java&#34;&gt;FileInputReader&lt;/a&gt;.
The FileInputReader is used by tasks to read scheduled source files.&lt;/p&gt;
&lt;h2 id=&#34;rowfileinputreader-default&#34;&gt;RowFileInputReader (default)&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;RowFileInputReader&lt;/code&gt; reads files from the local file system line by line.
This reader creates one record per row. It should be used for reading delimited text files, application log files, etc.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.RowFileInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/RowFileInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;bytesarrayinputreader&#34;&gt;BytesArrayInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;BytesArrayInputReader&lt;/code&gt; create a single byte array record from a source file.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.BytesArrayInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/BytesArrayInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;avrofileinputreader&#34;&gt;AvroFileInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;AvroFileInputReader&lt;/code&gt; is used to read Avro files.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.AvroFileInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/AvroFileInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;xmlfileinputreader&#34;&gt;XMLFileInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;XMLFileInputReader&lt;/code&gt; is used to read XML files.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.XMLFileInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/XMLFileInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;fileinputmetadatareader&#34;&gt;FileInputMetadataReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;FileInputMetadataReader&lt;/code&gt; is used to send a single record per file containing metadata (i.e: &lt;code&gt;name&lt;/code&gt;, &lt;code&gt;path&lt;/code&gt;, &lt;code&gt;hash&lt;/code&gt;, &lt;code&gt;lastModified&lt;/code&gt;, &lt;code&gt;size&lt;/code&gt;, etc)&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.FileInputMetadataReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/FileInputMetadataReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: File Readers</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/developer-guide/file-readers/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/developer-guide/file-readers/</guid>
      <description>
        
        
        &lt;p&gt;The connector can be configured with a specific &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-api/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/FileInputReader.java&#34;&gt;FileInputReader&lt;/a&gt;.
The FileInputReader is used by tasks to read scheduled source files.&lt;/p&gt;
&lt;h2 id=&#34;rowfileinputreader-default&#34;&gt;RowFileInputReader (default)&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;RowFileInputReader&lt;/code&gt; reads files from the local file system line by line.
This reader creates one record per row. It should be used for reading delimited text files, application log files, etc.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.RowFileInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/RowFileInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;bytesarrayinputreader&#34;&gt;BytesArrayInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;BytesArrayInputReader&lt;/code&gt; create a single byte array record from a source file.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.BytesArrayInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/BytesArrayInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;avrofileinputreader&#34;&gt;AvroFileInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;AvroFileInputReader&lt;/code&gt; is used to read Avro files.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.AvroFileInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/AvroFileInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;xmlfileinputreader&#34;&gt;XMLFileInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;XMLFileInputReader&lt;/code&gt; is used to read XML files.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.XMLFileInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/XMLFileInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;fileinputmetadatareader&#34;&gt;FileInputMetadataReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;FileInputMetadataReader&lt;/code&gt; is used to send a single record per file containing metadata (i.e: &lt;code&gt;name&lt;/code&gt;, &lt;code&gt;path&lt;/code&gt;, &lt;code&gt;hash&lt;/code&gt;, &lt;code&gt;lastModified&lt;/code&gt;, &lt;code&gt;size&lt;/code&gt;, etc)&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.FileInputMetadataReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/FileInputMetadataReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: File Readers</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/developer-guide/file-readers/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/developer-guide/file-readers/</guid>
      <description>
        
        
        &lt;p&gt;The connector can be configured with a specific &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-api/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/FileInputReader.java&#34;&gt;FileInputReader&lt;/a&gt;.
The FileInputReader is used by tasks to read scheduled source files.&lt;/p&gt;
&lt;h2 id=&#34;rowfileinputreader-default&#34;&gt;RowFileInputReader (default)&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;RowFileInputReader&lt;/code&gt; reads files from the local file system line by line.
This reader creates one record per row. It should be used for reading delimited text files, application log files, etc.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.RowFileInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/RowFileInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&#34;configuration&#34;&gt;Configuration&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;file.encoding&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The text file encoding to use&lt;/td&gt;
&lt;td&gt;&lt;code&gt;String&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;UTF_8&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;buffer.initial.bytes.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The initial buffer size used to read input files.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;String&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;4096&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;min.read.records&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The minimum number of records to read from file before returning to task.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Integer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;skip.headers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The number of rows to be skipped in the beginning of file.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Integer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;skip.footers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The number of rows to be skipped at the end of file.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Integer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;read.max.wait.ms&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The maximum time to wait in milliseconds for more bytes after hitting end of file.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Long&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;bytesarrayinputreader&#34;&gt;BytesArrayInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;BytesArrayInputReader&lt;/code&gt; create a single byte array record from a source file.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.BytesArrayInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/BytesArrayInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;avrofileinputreader&#34;&gt;AvroFileInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;AvroFileInputReader&lt;/code&gt; is used to read Avro files.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.AvroFileInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/AvroFileInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;xmlfileinputreader&#34;&gt;XMLFileInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;XMLFileInputReader&lt;/code&gt; is used to read XML files.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.XMLFileInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/XMLFileInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&#34;configuration1&#34;&gt;Configuration&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;xpath.expression&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The XPath expression used extract data from XML input files&lt;/td&gt;
&lt;td&gt;&lt;code&gt;String&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;xpath.result.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The expected result type for the XPath expression in [NODESET, STRING]&lt;/td&gt;
&lt;td&gt;&lt;code&gt;String&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;NODESET&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;force.array.on.fields&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The comma-separated list of fields for which an array-type must be forced&lt;/td&gt;
&lt;td&gt;&lt;code&gt;List&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;fileinputmetadatareader&#34;&gt;FileInputMetadataReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;FileInputMetadataReader&lt;/code&gt; is used to send a single record per file containing metadata (i.e: &lt;code&gt;name&lt;/code&gt;, &lt;code&gt;path&lt;/code&gt;, &lt;code&gt;hash&lt;/code&gt;, &lt;code&gt;lastModified&lt;/code&gt;, &lt;code&gt;size&lt;/code&gt;, etc)&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.FileInputMetadataReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/FileInputMetadataReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: File Readers</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/developer-guide/file-readers/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/developer-guide/file-readers/</guid>
      <description>
        
        
        &lt;p&gt;The connector can be configured with a specific &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-api/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/FileInputReader.java&#34;&gt;FileInputReader&lt;/a&gt;.
The FileInputReader is used by tasks to read scheduled source files.&lt;/p&gt;
&lt;h2 id=&#34;rowfileinputreader-default&#34;&gt;RowFileInputReader (default)&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;RowFileInputReader&lt;/code&gt; reads files from the local file system line by line.
This reader creates one record per row. It should be used for reading delimited text files, application log files, etc.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.RowFileInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/RowFileInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;bytesarrayinputreader&#34;&gt;BytesArrayInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;BytesArrayInputReader&lt;/code&gt; create a single byte array record from a source file.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.BytesArrayInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/BytesArrayInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;avrofileinputreader&#34;&gt;AvroFileInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;AvroFileInputReader&lt;/code&gt; is used to read Avro files.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.AvroFileInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/AvroFileInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;xmlfileinputreader&#34;&gt;XMLFileInputReader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;XMLFileInputReader&lt;/code&gt; is used to read XML files.&lt;/p&gt;
&lt;p&gt;The following provides usage information for &lt;code&gt;io.streamthoughts.kafka.connect.filepulse.reader.XMLFileInputReader&lt;/code&gt; (&lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-plugin/src/main/java/io/streamthoughts/kafka/connect/filepulse/reader/XMLFileInputReader.java&#34;&gt;source code&lt;/a&gt;)&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Identifying Files</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/offsets/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/offsets/</guid>
      <description>
        
        
        &lt;p&gt;Kafka Connect FilePulse uses a pluggable interface called &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-api/src/main/java/io/streamthoughts/kafka/connect/filepulse/source/SourceOffsetPolicy.java&#34;&gt;&lt;code&gt;SourceOffsetPolicy&lt;/code&gt;&lt;/a&gt; for
uniquely identifying files. Basically, the implementation passed in the connector&#39;s configuration is used for computing a unique identifier which is
used by Kafka Connect to persist the position of the connector for each file (i.e., the offsets saved in the &lt;code&gt;connect-offsets&lt;/code&gt; topic).&lt;/p&gt;
&lt;p&gt;By default, Kafka Connect FilePulse use the default implementation &lt;code&gt;DefaultSourceOffsetPolicy&lt;/code&gt; which accepts the following configuration:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;offset.attributes.string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A separated list of attributes, using &amp;lsquo;+&amp;rsquo; character as separator, to be used for uniquely identifying an object file; must be one of [name, path, lastModified, inode, hash, uri] (e.g: name+hash). Note that order doesn&#39;t matter.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;path+name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Identifying files</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/offsets/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/offsets/</guid>
      <description>
        
        
        &lt;p&gt;Kafka Connect FilePulse uses a pluggable interface called &lt;a href=&#34;https://github.com/streamthoughts/kafka-connect-file-pulse/blob/master/connect-file-pulse-api/src/main/java/io/streamthoughts/kafka/connect/filepulse/source/SourceOffsetPolicy.java&#34;&gt;&lt;code&gt;SourceOffsetPolicy&lt;/code&gt;&lt;/a&gt; for
uniquely identifying files. Basically, the implementation passed in the connector&#39;s configuration is used for computing a unique identifier which is
used by Kafka Connect to persist the position of the connector for each file (i.e., the offsets saved in the &lt;code&gt;connect-offsets&lt;/code&gt; topic).&lt;/p&gt;
&lt;p&gt;By default, Kafka Connect FilePulse use the default implementation &lt;code&gt;DefaultSourceOffsetPolicy&lt;/code&gt; which accepts the following configuration:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Importance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;offset.attributes.string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A separated list of attributes, using &amp;lsquo;+&amp;rsquo; character as separator, to be used for uniquely identifying an object file; must be one of [name, path, lastModified, inode, hash, uri] (e.g: name+hash). Note that order doesn&#39;t matter.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;path+name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Filter Chain Definition</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/developer-guide/filters-chain-definition/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/developer-guide/filters-chain-definition/</guid>
      <description>
        
        
        &lt;p&gt;The connector can be configured to apply complex transformations on messages before they are written to Kafka.&lt;/p&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;p&gt;A &lt;a href=&#34;#filters&#34;&gt;filter&lt;/a&gt; chain can be specified in the connector configuration.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;filters - List of aliases for the filter, specifying the order in which the filters will be applied.&lt;/li&gt;
&lt;li&gt;filters.$alias.type - Fully qualified class name for the filter.&lt;/li&gt;
&lt;li&gt;filters.$alias.$filterSpecificConfig Configuration properties for the filter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, let&#39;s parse a standard application logs file written with log4j using the build-in filters :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filters=GroupMultilineException, ExtractFirstLine, ParseLog4jLog

filters.GroupMultilineException.type=io.streamthoughts.kafka.connect.filepulse.filter.MultiRowFilter
filters.GroupMultilineException.negate=false
filters.GroupMultilineException.pattern=&amp;quot;^[\\t]&amp;quot;

filters.ExtractFirstLine.type=io.streamthoughts.kafka.connect.filepulse.filter.AppendFilter
filters.ExtractFirstLine.field=logmessage
filters.ExtractFirstLine.values=${extractarray(message,0)}

filters.ParseLog4jLog.type=io.streamthoughts.kafka.connect.filepulse.filter.impl.GrokFilter
filters.ParseLog4jLog.match=&amp;quot;%{TIMESTAMP_ISO8601:logdate} %{LOGLEVEL:loglevel} %{GREEDYDATA:thread} %{GREEDYDATA:logmessage}&amp;quot;
filters.ParseLog4jLog.source=log
filters.ParseLog4jLog.overwrite=logmessage
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;available-filters&#34;&gt;Available Filters&lt;/h2&gt;
&lt;p&gt;These filters are available for use with Kafka Connect File Pulse:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Filter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters#appendfilter&#34;&gt;AppendFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Appends one or more values to an existing or non-existing array field&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters#convertfilter&#34;&gt;ConvertFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Converts a message field&#39;s value to a specific type&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters#datefilter&#34;&gt;DateFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Converts a field&#39;s value containing a date to a unix epoch time&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;./kafka-connect-file-pulse/docs/developer-guide/filters#delimitedrowfilter&#34;&gt;DelimitedRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Parses a message field&#39;s value containing columns delimited by a separator into a struct&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters#dropfilter&#34;&gt;DropFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Drops messages satisfying a specific condition without throwing exception&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters#failfilter&#34;&gt;FailFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Throws an exception when a message satisfy a specific condition&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters#grokfilter&#34;&gt;GrokFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Parses an unstructured message field&#39;s value to a struct by combining Grok patterns&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters#grouprowfilter&#34;&gt;GroupRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Regroups multiple following messages into a single message by composing a grouping key&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters#joinfilter&#34;&gt;JoinFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Joins values of an array field with a specified separator&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters#jsonfilter&#34;&gt;JSONFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Unmarshallings a JSON message field&#39;s value to a complex struct&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters#multirowfilter&#34;&gt;MultiRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Combines following message lines into single one by combining patterns&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters#renamefilter&#34;&gt;RenameFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Renames a message field&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters#splitfilter&#34;&gt;SplitFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Splits a message field&#39;s value to array&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;difference-between-kafka-connect-single-message-transforms-smt-functionality&#34;&gt;Difference between Kafka Connect Single Message Transforms (SMT) functionality&lt;/h2&gt;
&lt;p&gt;Filters can be compared to Kafka Connect built-in &lt;a href=&#34;https://kafka.apache.org/documentation/#connect_transforms&#34;&gt;Transformers&lt;/a&gt;.
However, filters allow more complex pipelines to be built for structuring file data.
For example, they can be used to split one input message to multiple messages or to temporarily buffer consecutive messages in order to regroup them by fields or a pattern.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Filter Chain Definition</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/developer-guide/filters-chain-definition/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/developer-guide/filters-chain-definition/</guid>
      <description>
        
        
        &lt;p&gt;The connector can be configured to apply complex transformations on messages before they are written to Kafka.&lt;/p&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;p&gt;A &lt;a href=&#34;#filters&#34;&gt;filter&lt;/a&gt; chain can be specified in the connector configuration.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;filters - List of aliases for the filter, specifying the order in which the filters will be applied.&lt;/li&gt;
&lt;li&gt;filters.$alias.type - Fully qualified class name for the filter.&lt;/li&gt;
&lt;li&gt;filters.$alias.$filterSpecificConfig Configuration properties for the filter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, let&#39;s parse a standard application logs file written with log4j using the build-in filters :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filters=GroupMultilineException, ExtractFirstLine, ParseLog4jLog

filters.GroupMultilineException.type=io.streamthoughts.kafka.connect.filepulse.filter.MultiRowFilter
filters.GroupMultilineException.negate=false
filters.GroupMultilineException.pattern=&amp;quot;^[\\t]&amp;quot;

filters.ExtractFirstLine.type=io.streamthoughts.kafka.connect.filepulse.filter.AppendFilter
filters.ExtractFirstLine.field=$.logmessage
filters.ExtractFirstLine.values={{ extract_array($.message, 0) }

filters.ParseLog4jLog.type=io.streamthoughts.kafka.connect.filepulse.filter.impl.GrokFilter
filters.ParseLog4jLog.match=&amp;quot;%{TIMESTAMP_ISO8601:logdate} %{LOGLEVEL:loglevel} %{GREEDYDATA:thread} %{GREEDYDATA:logmessage}&amp;quot;
filters.ParseLog4jLog.source=log
filters.ParseLog4jLog.overwrite=logmessage
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;available-filters&#34;&gt;Available Filters&lt;/h2&gt;
&lt;p&gt;These filters are available for use with Kafka Connect File Pulse:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Filter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Since&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#appendfilter&#34;&gt;AppendFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Appends one or more values to an existing or non-existing array field&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#convertfilter&#34;&gt;ConvertFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Converts a message field&#39;s value to a specific type&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#datefilter&#34;&gt;DateFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Converts a field&#39;s value containing a date to a unix epoch time&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;./filters#delimitedrowfilter&#34;&gt;DelimitedRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Parses a message field&#39;s value containing columns delimited by a separator into a struct&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#dropfilter&#34;&gt;DropFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Drops messages satisfying a specific condition without throwing exception.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#excludeFilter&#34;&gt;ExcludeFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Excludes one or more fields from the input record.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v1.4.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#explodeFilter&#34;&gt;ExplodeFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Explodes an array or list field into separate records.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v1.4.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#failfilter&#34;&gt;FailFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Throws an exception when a message satisfy a specific condition&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#grokfilter&#34;&gt;GrokFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Parses an unstructured message field&#39;s value to a struct by combining Grok patterns&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#grouprowfilter&#34;&gt;GroupRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Regroups multiple following messages into a single message by composing a grouping key&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#joinfilter&#34;&gt;JoinFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Joins values of an array field with a specified separator&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#jsonfilter&#34;&gt;JSONFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Unmarshallings a JSON message field&#39;s value to a complex struct&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#multirowfilter&#34;&gt;MultiRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Combines following message lines into single one by combining patterns&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#renamefilter&#34;&gt;RenameFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Renames a message field&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#splitfilter&#34;&gt;SplitFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Splits a message field&#39;s value to array&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;difference-between-kafka-connect-single-message-transforms-smt-functionality&#34;&gt;Difference between Kafka Connect Single Message Transforms (SMT) functionality&lt;/h2&gt;
&lt;p&gt;Filters can be compared to Kafka Connect built-in &lt;a href=&#34;https://kafka.apache.org/documentation/#connect_transforms&#34;&gt;Transformers&lt;/a&gt;.
However, filters allow more complex pipelines to be built for structuring file data.
For example, they can be used to split one input message to multiple messages or to temporarily buffer consecutive messages in order to regroup them by fields or a pattern.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Filter Chain Definition</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/developer-guide/filters-chain-definition/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/developer-guide/filters-chain-definition/</guid>
      <description>
        
        
        &lt;p&gt;The connector can be configured to apply complex transformations on messages before they are written to Kafka.&lt;/p&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;p&gt;A &lt;a href=&#34;#filters&#34;&gt;filter&lt;/a&gt; chain can be specified in the connector configuration.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;filters - List of aliases for the filter, specifying the order in which the filters will be applied.&lt;/li&gt;
&lt;li&gt;filters.$alias.type - Fully qualified class name for the filter.&lt;/li&gt;
&lt;li&gt;filters.$alias.$filterSpecificConfig Configuration properties for the filter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, let&#39;s parse a standard application logs file written with log4j using the build-in filters :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filters=GroupMultilineException, ExtractFirstLine, ParseLog4jLog

filters.GroupMultilineException.type=io.streamthoughts.kafka.connect.filepulse.filter.MultiRowFilter
filters.GroupMultilineException.negate=false
filters.GroupMultilineException.pattern=&amp;quot;^[\\t]&amp;quot;

filters.ExtractFirstLine.type=io.streamthoughts.kafka.connect.filepulse.filter.AppendFilter
filters.ExtractFirstLine.field=$.logmessage
filters.ExtractFirstLine.values={{ extract_array($.message, 0) }

filters.ParseLog4jLog.type=io.streamthoughts.kafka.connect.filepulse.filter.impl.GrokFilter
filters.ParseLog4jLog.match=&amp;quot;%{TIMESTAMP_ISO8601:logdate} %{LOGLEVEL:loglevel} %{GREEDYDATA:thread} %{GREEDYDATA:logmessage}&amp;quot;
filters.ParseLog4jLog.source=log
filters.ParseLog4jLog.overwrite=logmessage
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;available-filters&#34;&gt;Available Filters&lt;/h2&gt;
&lt;p&gt;These filters are available for use with Kafka Connect File Pulse:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Filter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Since&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#appendfilter&#34;&gt;AppendFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Appends one or more values to an existing or non-existing array field&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#convertfilter&#34;&gt;ConvertFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Converts a message field&#39;s value to a specific type&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#datefilter&#34;&gt;DateFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Converts a field&#39;s value containing a date to a unix epoch time&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;./filters#delimitedrowfilter&#34;&gt;DelimitedRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Parses a message field&#39;s value containing columns delimited by a separator into a struct&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#dropfilter&#34;&gt;DropFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Drops messages satisfying a specific condition without throwing exception.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#excludeFilter&#34;&gt;ExcludeFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Excludes one or more fields from the input record.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v1.4.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#explodeFilter&#34;&gt;ExplodeFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Explodes an array or list field into separate records.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v1.4.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#failfilter&#34;&gt;FailFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Throws an exception when a message satisfy a specific condition&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#grokfilter&#34;&gt;GrokFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Parses an unstructured message field&#39;s value to a struct by combining Grok patterns&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#grouprowfilter&#34;&gt;GroupRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Regroups multiple following messages into a single message by composing a grouping key&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#joinfilter&#34;&gt;JoinFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Joins values of an array field with a specified separator&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#jsonfilter&#34;&gt;JSONFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Unmarshallings a JSON message field&#39;s value to a complex struct&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#multirowfilter&#34;&gt;MultiRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Combines following message lines into single one by combining patterns&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#renamefilter&#34;&gt;RenameFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Renames a message field&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#splitfilter&#34;&gt;SplitFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Splits a message field&#39;s value to array&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;difference-between-kafka-connect-single-message-transforms-smt-functionality&#34;&gt;Difference between Kafka Connect Single Message Transforms (SMT) functionality&lt;/h2&gt;
&lt;p&gt;Filters can be compared to Kafka Connect built-in &lt;a href=&#34;https://kafka.apache.org/documentation/#connect_transforms&#34;&gt;Transformers&lt;/a&gt;.
However, filters allow more complex pipelines to be built for structuring file data.
For example, they can be used to split one input message to multiple messages or to temporarily buffer consecutive messages in order to regroup them by fields or a pattern.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Filter Chain Definition</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/developer-guide/filters-chain-definition/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/developer-guide/filters-chain-definition/</guid>
      <description>
        
        
        &lt;p&gt;The connector can be configured to apply complex transformations on messages before they are written to Kafka.&lt;/p&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;p&gt;A &lt;a href=&#34;#filters&#34;&gt;filter&lt;/a&gt; chain can be specified in the connector configuration.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;filters - List of aliases for the filter, specifying the order in which the filters will be applied.&lt;/li&gt;
&lt;li&gt;filters.$alias.type - Fully qualified class name for the filter.&lt;/li&gt;
&lt;li&gt;filters.$alias.$filterSpecificConfig Configuration properties for the filter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, let&#39;s parse a standard application logs file written with log4j using the build-in filters :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filters=GroupMultilineException, ExtractFirstLine, ParseLog4jLog

filters.GroupMultilineException.type=io.streamthoughts.kafka.connect.filepulse.filter.MultiRowFilter
filters.GroupMultilineException.negate=false
filters.GroupMultilineException.pattern=&amp;quot;^[\\t]&amp;quot;

filters.ExtractFirstLine.type=io.streamthoughts.kafka.connect.filepulse.filter.AppendFilter
filters.ExtractFirstLine.field=$.logmessage
filters.ExtractFirstLine.values={{ extract_array($.message, 0) }

filters.ParseLog4jLog.type=io.streamthoughts.kafka.connect.filepulse.filter.impl.GrokFilter
filters.ParseLog4jLog.match=&amp;quot;%{TIMESTAMP_ISO8601:logdate} %{LOGLEVEL:loglevel} %{GREEDYDATA:thread} %{GREEDYDATA:logmessage}&amp;quot;
filters.ParseLog4jLog.source=log
filters.ParseLog4jLog.overwrite=logmessage
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;available-filters&#34;&gt;Available Filters&lt;/h2&gt;
&lt;p&gt;These filters are available for use with Kafka Connect File Pulse:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Filter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Since&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#appendfilter&#34;&gt;AppendFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Appends one or more values to an existing or non-existing array field&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#convertfilter&#34;&gt;ConvertFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Converts a message field&#39;s value to a specific type&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#datefilter&#34;&gt;DateFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Converts a field&#39;s value containing a date to a unix epoch time&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;./filters#delimitedrowfilter&#34;&gt;DelimitedRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Parses a message field&#39;s value containing columns delimited by a separator into a struct&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#dropfilter&#34;&gt;DropFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Drops messages satisfying a specific condition without throwing exception.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#excludeFilter&#34;&gt;ExcludeFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Excludes one or more fields from the input record.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v1.4.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#explodeFilter&#34;&gt;ExplodeFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Explodes an array or list field into separate records.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v1.4.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#failfilter&#34;&gt;FailFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Throws an exception when a message satisfy a specific condition&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#grokfilter&#34;&gt;GrokFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Parses an unstructured message field&#39;s value to a struct by combining Grok patterns&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#grouprowfilter&#34;&gt;GroupRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Regroups multiple following messages into a single message by composing a grouping key&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#joinfilter&#34;&gt;JoinFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Joins values of an array field with a specified separator&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#jsonfilter&#34;&gt;JSONFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Unmarshallings a JSON message field&#39;s value to a complex struct&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#multirowfilter&#34;&gt;MultiRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Combines following message lines into single one by combining patterns&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#renamefilter&#34;&gt;RenameFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Renames a message field&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#splitfilter&#34;&gt;SplitFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Splits a message field&#39;s value to array&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;difference-between-kafka-connect-single-message-transforms-smt-functionality&#34;&gt;Difference between Kafka Connect Single Message Transforms (SMT) functionality&lt;/h2&gt;
&lt;p&gt;Filters can be compared to Kafka Connect built-in &lt;a href=&#34;https://kafka.apache.org/documentation/#connect_transforms&#34;&gt;Transformers&lt;/a&gt;.
However, filters allow more complex pipelines to be built for structuring file data.
For example, they can be used to split one input message to multiple messages or to temporarily buffer consecutive messages in order to regroup them by fields or a pattern.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Filter Chain Definition</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/filters-chain-definition/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/filters-chain-definition/</guid>
      <description>
        
        
        &lt;p&gt;The connector can be configured to apply complex transformations on messages before they are written to Kafka.&lt;/p&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;p&gt;A &lt;a href=&#34;#filters&#34;&gt;filter&lt;/a&gt; chain can be specified in the connector configuration.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;filters - List of aliases for the filter, specifying the order in which the filters will be applied.&lt;/li&gt;
&lt;li&gt;filters.$alias.type - Fully qualified class name for the filter.&lt;/li&gt;
&lt;li&gt;filters.$alias.$filterSpecificConfig Configuration properties for the filter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, let&#39;s parse a standard application logs file written with log4j using the build-in filters :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filters=GroupMultilineException, ExtractFirstLine, ParseLog4jLog

filters.GroupMultilineException.type=io.streamthoughts.kafka.connect.filepulse.filter.MultiRowFilter
filters.GroupMultilineException.negate=false
filters.GroupMultilineException.pattern=&amp;quot;^[\\t]&amp;quot;

filters.ExtractFirstLine.type=io.streamthoughts.kafka.connect.filepulse.filter.AppendFilter
filters.ExtractFirstLine.field=$.logmessage
filters.ExtractFirstLine.values={{ extract_array($.message, 0) }

filters.ParseLog4jLog.type=io.streamthoughts.kafka.connect.filepulse.filter.impl.GrokFilter
filters.ParseLog4jLog.match=&amp;quot;%{TIMESTAMP_ISO8601:logdate} %{LOGLEVEL:loglevel} %{GREEDYDATA:thread} %{GREEDYDATA:logmessage}&amp;quot;
filters.ParseLog4jLog.source=log
filters.ParseLog4jLog.overwrite=logmessage
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;available-filters&#34;&gt;Available Filters&lt;/h2&gt;
&lt;p&gt;These filters are available for use with Kafka Connect File Pulse:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Filter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Since&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#appendfilter&#34;&gt;AppendFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Appends one or more values to an existing or non-existing array field&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#convertfilter&#34;&gt;ConvertFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Converts a message field&#39;s value to a specific type&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#datefilter&#34;&gt;DateFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Converts a field&#39;s value containing a date to a unix epoch time&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;./filters#delimitedrowfilter&#34;&gt;DelimitedRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Parses a message field&#39;s value containing columns delimited by a separator into a struct&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#dropfilter&#34;&gt;DropFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Drops messages satisfying a specific condition without throwing exception.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#excludeFilter&#34;&gt;ExcludeFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Excludes one or more fields from the input record.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v1.4.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#explodeFilter&#34;&gt;ExplodeFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Explodes an array or list field into separate records.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v1.4.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#failfilter&#34;&gt;FailFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Throws an exception when a message satisfy a specific condition&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#grokfilter&#34;&gt;GrokFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Parses an unstructured message field&#39;s value to a struct by combining Grok patterns&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#grouprowfilter&#34;&gt;GroupRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Regroups multiple following messages into a single message by composing a grouping key&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#joinfilter&#34;&gt;JoinFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Joins values of an array field with a specified separator&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#jsonfilter&#34;&gt;JSONFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Unmarshallings a JSON message field&#39;s value to a complex struct&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#multirowfilter&#34;&gt;MultiRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Combines following message lines into single one by combining patterns&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#renamefilter&#34;&gt;RenameFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Renames a message field&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#splitfilter&#34;&gt;SplitFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Splits a message field&#39;s value to array&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;difference-between-kafka-connect-single-message-transforms-smt-functionality&#34;&gt;Difference between Kafka Connect Single Message Transforms (SMT) functionality&lt;/h2&gt;
&lt;p&gt;Filters can be compared to Kafka Connect built-in &lt;a href=&#34;https://kafka.apache.org/documentation/#connect_transforms&#34;&gt;Transformers&lt;/a&gt;.
However, filters allow more complex pipelines to be built for structuring file data.
For example, they can be used to split one input message to multiple messages or to temporarily buffer consecutive messages in order to regroup them by fields or a pattern.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Filter Chain Definition</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/filters-chain-definition/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/filters-chain-definition/</guid>
      <description>
        
        
        &lt;p&gt;The connector can be configured to apply complex transformations on messages before they are written to Kafka.&lt;/p&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;p&gt;A &lt;a href=&#34;#filters&#34;&gt;filter&lt;/a&gt; chain can be specified in the connector configuration.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;filters - List of aliases for the filter, specifying the order in which the filters will be applied.&lt;/li&gt;
&lt;li&gt;filters.$alias.type - Fully qualified class name for the filter.&lt;/li&gt;
&lt;li&gt;filters.$alias.$filterSpecificConfig Configuration properties for the filter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, let&#39;s parse a standard application logs file written with log4j using the build-in filters :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filters=GroupMultilineException, ExtractFirstLine, ParseLog4jLog

filters.GroupMultilineException.type=io.streamthoughts.kafka.connect.filepulse.filter.MultiRowFilter
filters.GroupMultilineException.negate=false
filters.GroupMultilineException.pattern=&amp;quot;^[\\t]&amp;quot;

filters.ExtractFirstLine.type=io.streamthoughts.kafka.connect.filepulse.filter.AppendFilter
filters.ExtractFirstLine.field=$.logmessage
filters.ExtractFirstLine.values={{ extract_array($.message, 0) }

filters.ParseLog4jLog.type=io.streamthoughts.kafka.connect.filepulse.filter.impl.GrokFilter
filters.ParseLog4jLog.match=&amp;quot;%{TIMESTAMP_ISO8601:logdate} %{LOGLEVEL:loglevel} %{GREEDYDATA:thread} %{GREEDYDATA:logmessage}&amp;quot;
filters.ParseLog4jLog.source=log
filters.ParseLog4jLog.overwrite=logmessage
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;available-filters&#34;&gt;Available Filters&lt;/h2&gt;
&lt;p&gt;These filters are available for use with Kafka Connect File Pulse:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Filter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Since&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#appendfilter&#34;&gt;AppendFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Appends one or more values to an existing or non-existing array field&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#convertfilter&#34;&gt;ConvertFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Converts a message field&#39;s value to a specific type&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#datefilter&#34;&gt;DateFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Converts a field&#39;s value containing a date to a unix epoch time&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;./filters#delimitedrowfilter&#34;&gt;DelimitedRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Parses a message field&#39;s value containing columns delimited by a separator into a struct&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#dropfilter&#34;&gt;DropFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Drops messages satisfying a specific condition without throwing exception.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#excludeFilter&#34;&gt;ExcludeFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Excludes one or more fields from the input record.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v1.4.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#explodeFilter&#34;&gt;ExplodeFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Explodes an array or list field into separate records.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v1.4.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#failfilter&#34;&gt;FailFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Throws an exception when a message satisfy a specific condition&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#grokfilter&#34;&gt;GrokFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Parses an unstructured message field&#39;s value to a struct by combining Grok patterns&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#grouprowfilter&#34;&gt;GroupRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Regroups multiple following messages into a single message by composing a grouping key&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#joinfilter&#34;&gt;JoinFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Joins values of an array field with a specified separator&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#jsonfilter&#34;&gt;JSONFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Unmarshallings a JSON message field&#39;s value to a complex struct&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#multirowfilter&#34;&gt;MultiRowFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Combines following message lines into single one by combining patterns&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#renamefilter&#34;&gt;RenameFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Renames a message field&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../filters#splitfilter&#34;&gt;SplitFilter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Splits a message field&#39;s value to array&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;difference-between-kafka-connect-single-message-transforms-smt-functionality&#34;&gt;Difference between Kafka Connect Single Message Transforms (SMT) functionality&lt;/h2&gt;
&lt;p&gt;Filters can be compared to Kafka Connect built-in &lt;a href=&#34;https://kafka.apache.org/documentation/#connect_transforms&#34;&gt;Transformers&lt;/a&gt;.
However, filters allow more complex pipelines to be built for structuring file data.
For example, they can be used to split one input message to multiple messages or to temporarily buffer consecutive messages in order to regroup them by fields or a pattern.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Accessing Data and Metadata</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/accessing-data-and-metadata/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/accessing-data-and-metadata/</guid>
      <description>
        
        
        &lt;p&gt;Some filters (e.g : &lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters/#appendfilter&#34;&gt;AppendFilter&lt;/a&gt;) can be configured using &lt;em&gt;Simple Connect Expression Language&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Simple Connect Expression Language&lt;/em&gt; (ScEL for short) is an expression language based on regex that allows quick access and manipulating record fields and metadata.&lt;/p&gt;
&lt;p&gt;The syntaxes to define an expression are of the form : &lt;code&gt;&amp;lt;expression string&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;quot;{{ &amp;lt;expression string&amp;gt; }}&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;ScEL supports the following capabilities :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Literal expressions&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Field Selector&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nested Navigation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;String substitution&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functions&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;literal-expressions&#34;&gt;Literal expressions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;String : &lt;code&gt;&#39;Hello World&#39;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Number : &lt;code&gt;42&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Boolean: &lt;code&gt;True&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Nullable: &lt;code&gt;null&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;field-selector&#34;&gt;Field Selector&lt;/h2&gt;
&lt;p&gt;The expression language can be used to easily select one field from the input record :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$.username&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;nested-navigation&#34;&gt;Nested Navigation&lt;/h2&gt;
&lt;p&gt;To navigate down a struct value, just use a period to indicate a nested field value :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$.address.city&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;string-substitution&#34;&gt;String substitution&lt;/h2&gt;
&lt;p&gt;The expression language can be used to easily build a new string field that concatenate multiple ones :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;The user {{ $.username }} is living in city {{ $.address.city }}&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;function&#34;&gt;Function&lt;/h2&gt;
&lt;p&gt;The expression language support function call :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;The user {{ $.username }} is living in city {{ uppercase($.address.city) }}&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;dynamic-field-selector&#34;&gt;Dynamic Field Selector&lt;/h2&gt;
&lt;p&gt;String substitution can be used to dynamically select a field :&lt;/p&gt;
&lt;p&gt;The bellow example shows how to dynamically build a field selector by concatenating &lt;code&gt;$.&lt;/code&gt; and
the first element present in the array field &lt;code&gt;$.values&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{{ &#39;$.&#39;extract_array($.values, 0) }}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note the use of double-quotes to define a substitution expressions&lt;/p&gt;
&lt;h2 id=&#34;builtin-functions&#34;&gt;Built-in Functions&lt;/h2&gt;
&lt;p&gt;ScEL supports a number of predefined functions that can be used to apply a single transformation on a field.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Syntax&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;concat&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Concatenate two or more string expressions.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ concat(expr1, expr2, ...) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;concat_ws&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Concatenate two or more string expressions, using the specified separator between each.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ concat_ws(separator, prefix, suffix, expr1, expr2, ...) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;contains&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an array field&#39;s value contains the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ contains(array, &#39;value&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;converts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts a field&#39;s value into the specified type&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ converts(field, INTEGER) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ends_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value end with the specified string suffix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ ends_with(field, &#39;suffix&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;equals&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string or number fields&#39;s value equals the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ equals(field, value) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exists&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an the specified field exists&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ ends_with(field, value) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;extract_array&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns the element at the specified position of the specified array&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ extract_array(array, 0) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;hash&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Hash a given string expression, using murmur2 algorithm&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ hash(field_expr) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;is_null&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ is_null(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;length&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns the number of elements into an array of the length of an string field&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ length(array) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;lowercase&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts all of the characters in a string field&#39;s value to lower case&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ lowercase(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;matches&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value match the specified regex&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ matches(field, &#39;regex&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;md5&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Compute the MD5 hash of string expression&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ md5(field_expr) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nlv&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Sets a default value if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ length(array) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;replace_all &lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Replaces every subsequence of the field&#39;s value that matches the given pattern with the given replacement string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ replace_all(field, &#39;regex&#39;, &#39;replacement&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;split&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Split a string field&#39;s value into an array using the specified regex or character&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ split(field_expr, regex) }}&lt;/code&gt; or  &lt;code&gt;{{ split(field_expr, regex, limit) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;starts_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value start with the specified string prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ starts_with(field, &#39;prefix&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;trim&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Trims the spaces from the beginning and end of a string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ trim(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;uppercase&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts all of the characters in a string field&#39;s value to upper case&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ uppercase(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;uuid&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Create a Universally Unique Identifier (UUID)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ uuid() }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In addition, ScEL supports nested functions.&lt;/p&gt;
&lt;p&gt;For example, the following expression can be used to replace all whitespace characters after transforming our field&#39;s value into lowercase.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;replace_all(lowercase($.field), &#39;\\s&#39;, &#39;-&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Limitation&lt;/h4&gt;
Currently, FilePulse does not support user-defined functions (UDFs). So you cannot register your own functions to enrich the expression language.
&lt;/div&gt;

&lt;h2 id=&#34;scopes&#34;&gt;Scopes&lt;/h2&gt;
&lt;p&gt;In the previous section, we have shown how to use the expression language to select a specific field.
The selected field was part of our the current record being processed.&lt;/p&gt;
&lt;p&gt;Actually, ScEL allows you to get access to additional fields through the used of scopes.
Basically, a scope defined the root object on which a selector expression must evaluated.&lt;/p&gt;
&lt;p&gt;The syntax to define an expression with a scope is of the form : &amp;ldquo;&lt;code&gt;$&amp;lt;scope&amp;gt;.&amp;lt;selector expression string&amp;gt;&lt;/code&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;By default, if no scope is defined in the expression, the scope &lt;code&gt;$value&lt;/code&gt; is implicitly used.&lt;/p&gt;
&lt;p&gt;ScEL supports a number of predefined scopes that can be used for example :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;To define the topic for the record.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To define the key for the record.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To get access to metadata about the source file.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Scope&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$headers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record headers&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$key&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record key&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file metadata&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The offset information of this record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$system&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment variables and runtime properties&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$timestamp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record timestamp&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The output topic&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$value&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$variables&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The contextual filter-chain variables&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, object]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note, that in case of failures more fields are added to the current filter context (see : &lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/handling-failures/&#34;&gt;Handling Failures&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&#34;record-headers&#34;&gt;Record Headers&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;headers&lt;/code&gt; allows defining the headers of the output record.&lt;/p&gt;
&lt;h3 id=&#34;record-key&#34;&gt;Record key&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;key&lt;/code&gt; allows defining the key of the output record. Only string key is currently supported.&lt;/p&gt;
&lt;h3 id=&#34;source-metadata&#34;&gt;Source Metadata&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;metadata&lt;/code&gt; allows read access to information about the file being processing.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file directory path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.absolutePath&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file absolute path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.hash&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file CRC32 hash&lt;/td&gt;
&lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.lastModified&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file last modified time.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file size&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.inode&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file Unix inode&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;record-offset&#34;&gt;Record Offset&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;offset&lt;/code&gt; allows read access to information about the original position of the record into the source file.
The available fields depend on the configured FileInputRecord.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.timestamp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The creation time of the record (millisecond)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;RowFilterReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.startPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.endPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The end position of the record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The size in bytes&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.row&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The row number of the record into the source&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;BytesArrayInputReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.startPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the record into the source file (always equals to 0)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.endPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The end position of the record into the source file (equals to the file size)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;AvroFilterInputReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.blockStart&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the current block&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.position&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The position into the current block.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.records&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The number of record read into the current block.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;system&#34;&gt;System&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;system&lt;/code&gt; allows accessing to the system environment variables and runtime properties.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$system.env&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment variables.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, string]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$system.props&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment properties.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, string]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;timestamp&#34;&gt;Timestamp&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$timestamp&lt;/code&gt; allows defining the timestamp of the output record.&lt;/p&gt;
&lt;h2 id=&#34;topic&#34;&gt;Topic&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$topic&lt;/code&gt; allows defining the target topic of the output record.&lt;/p&gt;
&lt;h2 id=&#34;value&#34;&gt;Value&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$value&lt;/code&gt; allows defining the fields of the output record&lt;/p&gt;
&lt;h2 id=&#34;variables&#34;&gt;Variables&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$variables&lt;/code&gt; allows read/write access to a simple key-value map structure.
This scope can be used to share user-defined variables between &lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters/&#34;&gt;Processing Filters&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note : variables are not cached between records.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Accessing Data and Metadata</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/accessing-data-and-metadata/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/accessing-data-and-metadata/</guid>
      <description>
        
        
        &lt;p&gt;Some filters (e.g : &lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters/#appendfilter&#34;&gt;AppendFilter&lt;/a&gt;) can be configured using &lt;em&gt;Simple Connect Expression Language&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Simple Connect Expression Language&lt;/em&gt; (ScEL for short) is an expression language based on regex that allows quick access and manipulating record fields and metadata.&lt;/p&gt;
&lt;p&gt;The syntaxes to define an expression are of the form : &lt;code&gt;&amp;lt;expression string&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;quot;{{ &amp;lt;expression string&amp;gt; }}&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;ScEL supports the following capabilities :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Literal expressions&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Field Selector&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nested Navigation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;String substitution&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functions&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;literal-expressions&#34;&gt;Literal expressions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;String : &lt;code&gt;&#39;Hello World&#39;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Number : &lt;code&gt;42&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Boolean: &lt;code&gt;True&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Nullable: &lt;code&gt;null&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;field-selector&#34;&gt;Field Selector&lt;/h2&gt;
&lt;p&gt;The expression language can be used to easily select one field from the input record :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$.username&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;nested-navigation&#34;&gt;Nested Navigation&lt;/h2&gt;
&lt;p&gt;To navigate down a struct value, just use a period to indicate a nested field value :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$.address.city&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;string-substitution&#34;&gt;String substitution&lt;/h2&gt;
&lt;p&gt;The expression language can be used to easily build a new string field that concatenate multiple ones :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;The user {{ $.username }} is living in city {{ $.address.city }}&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;function&#34;&gt;Function&lt;/h2&gt;
&lt;p&gt;The expression language support function call :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;The user {{ $.username }} is living in city {{ uppercase($.address.city) }}&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;dynamic-field-selector&#34;&gt;Dynamic Field Selector&lt;/h2&gt;
&lt;p&gt;String substitution can be used to dynamically select a field :&lt;/p&gt;
&lt;p&gt;The bellow example shows how to dynamically build a field selector by concatenating &lt;code&gt;$.&lt;/code&gt; and
the first element present in the array field &lt;code&gt;$.values&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{{ &#39;$.&#39;extract_array($.values, 0) }}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note the use of double-quotes to define a substitution expressions&lt;/p&gt;
&lt;h2 id=&#34;builtin-functions&#34;&gt;Built-in Functions&lt;/h2&gt;
&lt;p&gt;ScEL supports a number of predefined functions that can be used to apply a single transformation on a field.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Syntax&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;concat&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Concatenate two or more string expressions.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ concat(expr1, expr2, ...) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;concat_ws&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Concatenate two or more string expressions, using the specified separator between each.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ concat_ws(separator, prefix, suffix, expr1, expr2, ...) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;contains&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an array field&#39;s value contains the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ contains(array, &#39;value&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;converts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts a field&#39;s value into the specified type&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ converts(field_expr, INTEGER) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ends_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a string field&#39;s value end with the specified string suffix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ ends_with(field_expr, &#39;suffix&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;equals&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a string or number fields&#39;s value equals the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ equals(field_expr, value) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exists&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an object has the specified field&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ exists(obj_expr, field_expr) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;extract_array&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns the element at the specified position of the specified array&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ extract_array(array, 0) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;hash&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Hash a given string expression, using murmur2 algorithm&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ hash(field_expr) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;is_null&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ is_null(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;length&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns the number of elements into an array of the length of an string field&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ length(array) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;lowercase&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts all of the characters in a string field&#39;s value to lower case&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ lowercase(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;matches&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value match the specified regex&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ matches(field_expr, &#39;regex&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;md5&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Compute the MD5 hash of string expression&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ md5(field_expr) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nlv&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Sets a default value if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ length(array) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;replace_all &lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Replaces every subsequence of the field&#39;s value that matches the given pattern with the given replacement string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ replace_all(field_expr, &#39;regex&#39;, &#39;replacement&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;split&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Split a string field&#39;s value into an array using the specified regex or character&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ split(field_expr, regex) }}&lt;/code&gt; or  &lt;code&gt;{{ split(field_expr, regex, limit) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;starts_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value start with the specified string prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ starts_with(field_expr, &#39;prefix&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;trim&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Trims the spaces from the beginning and end of a string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ trim(field_expr) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;uppercase&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts all of the characters in a string field&#39;s value to upper case&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ uppercase(field_expr) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;uuid&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Create a Universally Unique Identifier (UUID)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ uuid() }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In addition, ScEL supports nested functions.&lt;/p&gt;
&lt;p&gt;For example, the following expression can be used to replace all whitespace characters after transforming our field&#39;s value into lowercase.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;replace_all(lowercase($.field), &#39;\\s&#39;, &#39;-&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Limitation&lt;/h4&gt;
Currently, FilePulse does not support user-defined functions (UDFs). So you cannot register your own functions to enrich the expression language.
&lt;/div&gt;

&lt;h2 id=&#34;scopes&#34;&gt;Scopes&lt;/h2&gt;
&lt;p&gt;In the previous section, we have shown how to use the expression language to select a specific field.
The selected field was part of our the current record being processed.&lt;/p&gt;
&lt;p&gt;Actually, ScEL allows you to get access to additional fields through the used of scopes.
Basically, a scope defined the root object on which a selector expression must evaluated.&lt;/p&gt;
&lt;p&gt;The syntax to define an expression with a scope is of the form : &amp;ldquo;&lt;code&gt;$&amp;lt;scope&amp;gt;.&amp;lt;selector expression string&amp;gt;&lt;/code&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;By default, if no scope is defined in the expression, the scope &lt;code&gt;$value&lt;/code&gt; is implicitly used.&lt;/p&gt;
&lt;p&gt;ScEL supports a number of predefined scopes that can be used for example :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;To define the topic for the record.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To define the key for the record.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To get access to metadata about the source file.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Scope&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$headers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record headers&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$key&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record key&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file metadata&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The offset information of this record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$system&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment variables and runtime properties&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$timestamp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record timestamp&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The output topic&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$value&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$variables&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The contextual filter-chain variables&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, object]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note, that in case of failures more fields are added to the current filter context (see : &lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/handling-failures/&#34;&gt;Handling Failures&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&#34;record-headers&#34;&gt;Record Headers&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;headers&lt;/code&gt; allows defining the headers of the output record.&lt;/p&gt;
&lt;h3 id=&#34;record-key&#34;&gt;Record key&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;key&lt;/code&gt; allows defining the key of the output record. Only string key is currently supported.&lt;/p&gt;
&lt;h3 id=&#34;source-metadata&#34;&gt;Source Metadata&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;metadata&lt;/code&gt; allows read access to information about the file being processing.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file directory path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.absolutePath&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file absolute path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.hash&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file CRC32 hash&lt;/td&gt;
&lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.lastModified&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file last modified time.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file size&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.inode&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file Unix inode&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;record-offset&#34;&gt;Record Offset&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;offset&lt;/code&gt; allows read access to information about the original position of the record into the source file.
The available fields depend on the configured FileInputRecord.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.timestamp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The creation time of the record (millisecond)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;RowFilterReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.startPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.endPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The end position of the record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The size in bytes&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.row&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The row number of the record into the source&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;BytesArrayInputReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.startPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the record into the source file (always equals to 0)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.endPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The end position of the record into the source file (equals to the file size)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;AvroFilterInputReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.blockStart&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the current block&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.position&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The position into the current block.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.records&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The number of record read into the current block.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;system&#34;&gt;System&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;system&lt;/code&gt; allows accessing to the system environment variables and runtime properties.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$system.env&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment variables.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, string]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$system.props&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment properties.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, string]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;timestamp&#34;&gt;Timestamp&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$timestamp&lt;/code&gt; allows defining the timestamp of the output record.&lt;/p&gt;
&lt;h2 id=&#34;topic&#34;&gt;Topic&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$topic&lt;/code&gt; allows defining the target topic of the output record.&lt;/p&gt;
&lt;h2 id=&#34;value&#34;&gt;Value&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$value&lt;/code&gt; allows defining the fields of the output record&lt;/p&gt;
&lt;h2 id=&#34;variables&#34;&gt;Variables&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$variables&lt;/code&gt; allows read/write access to a simple key-value map structure.
This scope can be used to share user-defined variables between &lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters/&#34;&gt;Processing Filters&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note : variables are not cached between records.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Accessing Data and Metadata</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/developer-guide/accessing-data-and-metadata/</link>
      <pubDate>Wed, 11 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/developer-guide/accessing-data-and-metadata/</guid>
      <description>
        
        
        &lt;p&gt;Some filters (e.g : &lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters/#appendfilter&#34;&gt;AppendFilter&lt;/a&gt; can be configured using &lt;em&gt;Simple Connect Expression Language&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Simple Connect Expression Language&lt;/em&gt; (ScEL for short) is an expression language based on regex that allows quick access and manipulating record fields and metadata.&lt;/p&gt;
&lt;p&gt;The syntaxes to define an expression are of the form : &lt;code&gt;&amp;lt;expression string&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;quot;{{ &amp;lt;expression string&amp;gt; }}&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;ScEL supports the following capabilities :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Literal expressions&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Field Selector&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nested Navigation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;String substitution&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functions&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;literal-expressions&#34;&gt;Literal expressions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;String : &lt;code&gt;&#39;Hello World&#39;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Number : &lt;code&gt;42&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Boolean: &lt;code&gt;True&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Nullable: &lt;code&gt;null&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;field-selector&#34;&gt;Field Selector&lt;/h2&gt;
&lt;p&gt;The expression language can be used to easily select one field from the input record :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$.username&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;nested-navigation&#34;&gt;Nested Navigation&lt;/h2&gt;
&lt;p&gt;To navigate down a struct value, just use a period to indicate a nested field value :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$.address.city&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;string-substitution&#34;&gt;String substitution&lt;/h2&gt;
&lt;p&gt;The expression language can be used to easily build a new string field that concatenate multiple ones :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;The user {{ $.username }} is living in city {{ $.address.city }}&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;function&#34;&gt;Function&lt;/h2&gt;
&lt;p&gt;The expression language support function call :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;The user {{ $.username }} is living in city {{ uppercase($.address.city) }}&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;dynamic-field-selector&#34;&gt;Dynamic Field Selector&lt;/h2&gt;
&lt;p&gt;String substitution can be used to dynamically select a field :&lt;/p&gt;
&lt;p&gt;The bellow example shows how to dynamically build a field selector by concatenating &lt;code&gt;$.&lt;/code&gt; and
the first element present in the array field &lt;code&gt;$.values&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{{ &#39;$.&#39;extract_array($.values, 0) }}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note the used of double-quotes to define a substitution expressions&lt;/p&gt;
&lt;h2 id=&#34;builtin-functions&#34;&gt;Built-in Functions&lt;/h2&gt;
&lt;p&gt;ScEL supports a number of predefined functions that can be used to apply a single transformation on a field.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Syntax&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;concat&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Concatenate two or more string expressions.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ concat(expr1, expr2, ...) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;concat_ws&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Concatenate two or more string expressions, using the specified separator between each.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ concat_ws(separator, prefix, suffix, expr1, expr2, ...) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;contains&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an array field&#39;s value contains the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ contains(array, &#39;value&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;converts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts a field&#39;s value into the specified type&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ converts(field, INTEGER) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ends_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value end with the specified string suffix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ ends_with(field, &#39;suffix&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;equals&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string or number fields&#39;s value equals the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ equals(field, value) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exists&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an the specified field exists&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ ends_with(field, value) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;extract_array&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns the element at the specified position of the specified array&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ extract_array(array, 0) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;hash&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Hash a given string expression, using murmur2 algorithm&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ hash(field_expr) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;is_null&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ is_null(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;length&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns the number of elements into an array of the length of an string field&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ length(array) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;lowercase&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts all of the characters in a string field&#39;s value to lower case&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ lowercase(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;matches&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value match the specified regex&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ matches(field, &#39;regex&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;md5&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Compute the MD5 hash of string expression&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ md5(field_expr) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nlv&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Sets a default value if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ length(array) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;replace_all &lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Replaces every subsequence of the field&#39;s value that matches the given pattern with the given replacement string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ replace_all(field, &#39;regex&#39;, &#39;replacement&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;starts_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value start with the specified string prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ starts_with(field, &#39;prefix&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;trim&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Trims the spaces from the beginning and end of a string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ trim(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;uppercase&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts all of the characters in a string field&#39;s value to upper case&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ uppercase(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;uuid&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Create a Universally Unique Identifier (UUID)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ uuid() }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In addition, ScEL supports nested functions.&lt;/p&gt;
&lt;p&gt;For example, the following expression can be used to replace all whitespace characters after transforming our field&#39;s value into lowercase.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;replace_all(lowercase($.field), &#39;\\s&#39;, &#39;-&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Limitation&lt;/h4&gt;
Currently, FilePulse does not support user-defined functions (UDFs). So you cannot register your own functions to enrich the expression language.
&lt;/div&gt;

&lt;h2 id=&#34;scopes&#34;&gt;Scopes&lt;/h2&gt;
&lt;p&gt;In the previous section, we have shown how to use the expression language to select a specific field.
The selected field was part of our the current record being processed.&lt;/p&gt;
&lt;p&gt;Actually, ScEL allows you to get access to additional fields through the used of scopes.
Basically, a scope defined the root object on which a selector expression must evaluated.&lt;/p&gt;
&lt;p&gt;The syntax to define an expression with a scope is of the form : &amp;ldquo;&lt;code&gt;$&amp;lt;scope&amp;gt;.&amp;lt;selector expression string&amp;gt;&lt;/code&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;By default, if no scope is defined in the expression, the scope &lt;code&gt;$value&lt;/code&gt; is implicitly used.&lt;/p&gt;
&lt;p&gt;ScEL supports a number of predefined scopes that can be used for example :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;To define the topic for the record.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To define the key for the record.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To get access to metadata about the source file.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Scope&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$headers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record headers&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$key&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record key&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file metadata&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The offset information of this record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$system&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment variables and runtime properties&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$timestamp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record timestamp&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The output topic&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$value&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$variables&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The contextual filter-chain variables&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, object]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note, that in case of failures more fields are added to the current filter context (see : &lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/handling-failures/&#34;&gt;Handling Failures&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&#34;record-headers&#34;&gt;Record Headers&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;headers&lt;/code&gt; allows defining the headers of the output record.&lt;/p&gt;
&lt;h3 id=&#34;record-key&#34;&gt;Record key&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;key&lt;/code&gt; allows defining the key of the output record. Only string key is currently supported.&lt;/p&gt;
&lt;h3 id=&#34;source-metadata&#34;&gt;Source Metadata&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;metadata&lt;/code&gt; allows read access to information about the file being processing.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file directory path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.absolutePath&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file absolute path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.hash&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file CRC32 hash&lt;/td&gt;
&lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.lastModified&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file last modified time.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file size&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.inode&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file Unix inode&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;record-offset&#34;&gt;Record Offset&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;offset&lt;/code&gt; allows read access to information about the original position of the record into the source file.
The available fields depend on the configured FileInputRecord.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.timestamp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The creation time of the record (millisecond)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;RowFilterReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.startPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.endPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The end position of the record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The size in bytes&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.row&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The row number of the record into the source&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;BytesArrayInputReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.startPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the record into the source file (always equals to 0)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.endPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The end position of the record into the source file (equals to the file size)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;AvroFilterInputReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.blockStart&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the current block&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.position&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The position into the current block.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.records&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The number of record read into the current block.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;system&#34;&gt;System&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;system&lt;/code&gt; allows accessing to the system environment variables and runtime properties.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$system.env&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment variables.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, string]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$system.props&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment properties.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, string]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;timestamp&#34;&gt;Timestamp&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$timestamp&lt;/code&gt; allows defining the timestamp of the output record.&lt;/p&gt;
&lt;h2 id=&#34;topic&#34;&gt;Topic&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$topic&lt;/code&gt; allows defining the target topic of the output record.&lt;/p&gt;
&lt;h2 id=&#34;value&#34;&gt;Value&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$value&lt;/code&gt; allows defining the fields of the output record&lt;/p&gt;
&lt;h2 id=&#34;variables&#34;&gt;Variables&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$variables&lt;/code&gt; allows read/write access to a simple key-value map structure.
This scope can be used to share user-defined variables between &lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters/&#34;&gt;Processing Filters&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note : variables are not cached between records.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Accessing Data and Metadata</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/developer-guide/accessing-data-and-metadata/</link>
      <pubDate>Wed, 11 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/developer-guide/accessing-data-and-metadata/</guid>
      <description>
        
        
        &lt;p&gt;Some filters (e.g : &lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters/#appendfilter&#34;&gt;AppendFilter&lt;/a&gt; can be configured using &lt;em&gt;Simple Connect Expression Language&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Simple Connect Expression Language&lt;/em&gt; (ScEL for short) is an expression language based on regex that allows quick access and manipulating record fields and metadata.&lt;/p&gt;
&lt;p&gt;The syntaxes to define an expression are of the form : &lt;code&gt;&amp;lt;expression string&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;quot;{{ &amp;lt;expression string&amp;gt; }}&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;ScEL supports the following capabilities :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Literal expressions&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Field Selector&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nested Navigation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;String substitution&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functions&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;literal-expressions&#34;&gt;Literal expressions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;String : &lt;code&gt;&#39;Hello World&#39;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Number : &lt;code&gt;42&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Boolean: &lt;code&gt;True&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Nullable: &lt;code&gt;null&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;field-selector&#34;&gt;Field Selector&lt;/h2&gt;
&lt;p&gt;The expression language can be used to easily select one field from the input record :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$.username&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;nested-navigation&#34;&gt;Nested Navigation&lt;/h2&gt;
&lt;p&gt;To navigate down a struct value, just use a period to indicate a nested field value :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$.address.city&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;string-substitution&#34;&gt;String substitution&lt;/h2&gt;
&lt;p&gt;The expression language can be used to easily build a new string field that concatenate multiple ones :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;The user {{ $.username }} is living in city {{ $.address.city }}&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;function&#34;&gt;Function&lt;/h2&gt;
&lt;p&gt;The expression language support function call :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;The user {{ $.username }} is living in city {{ uppercase($.address.city) }}&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;dynamic-field-selector&#34;&gt;Dynamic Field Selector&lt;/h2&gt;
&lt;p&gt;String substitution can be used to dynamically select a field :&lt;/p&gt;
&lt;p&gt;The bellow example shows how to dynamically build a field selector by concatenating &lt;code&gt;$.&lt;/code&gt; and
the first element present in the array field &lt;code&gt;$.values&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{{ &#39;$.&#39;extract_array($.values, 0) }}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note the used of double-quotes to define a substitution expressions&lt;/p&gt;
&lt;h2 id=&#34;builtin-functions&#34;&gt;Built-in Functions&lt;/h2&gt;
&lt;p&gt;ScEL supports a number of predefined functions that can be used to apply a single transformation on a field.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Syntax&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;concat&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Concatenate two or more string expressions.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ concat(expr1, expr2, ...) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;concat_ws&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Concatenate two or more string expressions, using the specified separator between each.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ concat_ws(separator, prefix, suffix, expr1, expr2, ...) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;contains&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an array field&#39;s value contains the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ contains(array, &#39;value&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;converts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts a field&#39;s value into the specified type&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ converts(field, INTEGER) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ends_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value end with the specified string suffix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ ends_with(field, &#39;suffix&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;equals&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string or number fields&#39;s value equals the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ equals(field, value) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exists&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an the specified field exists&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ ends_with(field, value) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;extract_array&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns the element at the specified position of the specified array&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ extract_array(array, 0) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;hash&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Hash a given string expression, using murmur2 algorithm&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ hash(field_expr) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;is_null&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ is_null(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;length&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns the number of elements into an array of the length of an string field&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ length(array) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;lowercase&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts all of the characters in a string field&#39;s value to lower case&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ lowercase(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;matches&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value match the specified regex&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ matches(field, &#39;regex&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;md5&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Compute the MD5 hash of string expression&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ md5(field_expr) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nlv&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Sets a default value if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ length(array) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;replace_all &lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Replaces every subsequence of the field&#39;s value that matches the given pattern with the given replacement string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ replace_all(field, &#39;regex&#39;, &#39;replacement&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;split&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Split a string field&#39;s value into an array using the specified regex or character&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ split(field_expr, regex) }}&lt;/code&gt; or  &lt;code&gt;{{ split(field_expr, regex, limit) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;starts_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value start with the specified string prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ starts_with(field, &#39;prefix&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;trim&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Trims the spaces from the beginning and end of a string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ trim(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;uppercase&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts all of the characters in a string field&#39;s value to upper case&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ uppercase(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;uuid&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Create a Universally Unique Identifier (UUID)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ uuid() }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In addition, ScEL supports nested functions.&lt;/p&gt;
&lt;p&gt;For example, the following expression can be used to replace all whitespace characters after transforming our field&#39;s value into lowercase.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;replace_all(lowercase($.field), &#39;\\s&#39;, &#39;-&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Limitation&lt;/h4&gt;
Currently, FilePulse does not support user-defined functions (UDFs). So you cannot register your own functions to enrich the expression language.
&lt;/div&gt;

&lt;h2 id=&#34;scopes&#34;&gt;Scopes&lt;/h2&gt;
&lt;p&gt;In the previous section, we have shown how to use the expression language to select a specific field.
The selected field was part of our the current record being processed.&lt;/p&gt;
&lt;p&gt;Actually, ScEL allows you to get access to additional fields through the used of scopes.
Basically, a scope defined the root object on which a selector expression must evaluated.&lt;/p&gt;
&lt;p&gt;The syntax to define an expression with a scope is of the form : &amp;ldquo;&lt;code&gt;$&amp;lt;scope&amp;gt;.&amp;lt;selector expression string&amp;gt;&lt;/code&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;By default, if no scope is defined in the expression, the scope &lt;code&gt;$value&lt;/code&gt; is implicitly used.&lt;/p&gt;
&lt;p&gt;ScEL supports a number of predefined scopes that can be used for example :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;To define the topic for the record.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To define the key for the record.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To get access to metadata about the source file.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Scope&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$headers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record headers&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$key&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record key&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file metadata&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The offset information of this record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$system&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment variables and runtime properties&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$timestamp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record timestamp&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The output topic&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$value&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$variables&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The contextual filter-chain variables&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, object]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note, that in case of failures more fields are added to the current filter context (see : &lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/handling-failures/&#34;&gt;Handling Failures&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&#34;record-headers&#34;&gt;Record Headers&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;headers&lt;/code&gt; allows defining the headers of the output record.&lt;/p&gt;
&lt;h3 id=&#34;record-key&#34;&gt;Record key&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;key&lt;/code&gt; allows defining the key of the output record. Only string key is currently supported.&lt;/p&gt;
&lt;h3 id=&#34;source-metadata&#34;&gt;Source Metadata&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;metadata&lt;/code&gt; allows read access to information about the file being processing.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file directory path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.absolutePath&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file absolute path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.hash&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file CRC32 hash&lt;/td&gt;
&lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.lastModified&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file last modified time.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file size&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.inode&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file Unix inode&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;record-offset&#34;&gt;Record Offset&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;offset&lt;/code&gt; allows read access to information about the original position of the record into the source file.
The available fields depend on the configured FileInputRecord.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.timestamp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The creation time of the record (millisecond)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;RowFilterReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.startPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.endPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The end position of the record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The size in bytes&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.row&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The row number of the record into the source&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;BytesArrayInputReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.startPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the record into the source file (always equals to 0)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.endPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The end position of the record into the source file (equals to the file size)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;AvroFilterInputReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.blockStart&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the current block&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.position&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The position into the current block.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.records&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The number of record read into the current block.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;system&#34;&gt;System&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;system&lt;/code&gt; allows accessing to the system environment variables and runtime properties.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$system.env&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment variables.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, string]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$system.props&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment properties.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, string]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;timestamp&#34;&gt;Timestamp&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$timestamp&lt;/code&gt; allows defining the timestamp of the output record.&lt;/p&gt;
&lt;h2 id=&#34;topic&#34;&gt;Topic&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$topic&lt;/code&gt; allows defining the target topic of the output record.&lt;/p&gt;
&lt;h2 id=&#34;value&#34;&gt;Value&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$value&lt;/code&gt; allows defining the fields of the output record&lt;/p&gt;
&lt;h2 id=&#34;variables&#34;&gt;Variables&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$variables&lt;/code&gt; allows read/write access to a simple key-value map structure.
This scope can be used to share user-defined variables between &lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters/&#34;&gt;Processing Filters&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note : variables are not cached between records.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Accessing Data and Metadata</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/developer-guide/accessing-data-and-metadata/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/developer-guide/accessing-data-and-metadata/</guid>
      <description>
        
        
        &lt;p&gt;Some filters (e.g : &lt;a href=&#34;#appendfilter&#34;&gt;AppendFilter&lt;/a&gt;) can be configured using &lt;em&gt;Simple Connect Expression Language&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Simple Connect Expression Language&lt;/em&gt; (ScEL for short) is an expression language based on regex that allows quick access and manipulating record fields and metadata.&lt;/p&gt;
&lt;p&gt;The synthaxes to define an expression are of the form : &lt;code&gt;&amp;lt;expression string&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;quot;{{ &amp;lt;expression string&amp;gt; }}&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;ScEL supports the following capabilities :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Literal expressions&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Field Selector&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nested Navigation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;String substitution&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functions&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;literal-expressions&#34;&gt;Literal expressions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;String : &lt;code&gt;&#39;Hello World&#39;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Number : &lt;code&gt;42&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Boolean: &lt;code&gt;True&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Nullable: &lt;code&gt;null&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;field-selector&#34;&gt;Field Selector&lt;/h2&gt;
&lt;p&gt;The expression language can be used to easily select one field from the input record :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$.username&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;nested-navigation&#34;&gt;Nested Navigation&lt;/h2&gt;
&lt;p&gt;To navigate down a struct value, just use a period to indicate a nested field value :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$.address.city&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;string-substitution&#34;&gt;String substitution&lt;/h2&gt;
&lt;p&gt;The expression language can be used to easily build a new string field that concatenate multiple ones :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;The user {{ $.username }} is living in city {{ $.address.city }}&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;function&#34;&gt;Function&lt;/h2&gt;
&lt;p&gt;The expression language support function call :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;The user {{ $.username }} is living in city {{ uppercase($.address.city) }}&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;dynamic-field-selector&#34;&gt;Dynamic Field Selector&lt;/h2&gt;
&lt;p&gt;String substitution can be used to dynamically select a field :&lt;/p&gt;
&lt;p&gt;The bellow example shows how to dynamically build a field selector by concatenating &lt;code&gt;$.&lt;/code&gt; and
the first element present in the array field &lt;code&gt;$.values&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{{ &#39;$.&#39;extract_array($.values, 0) }}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note the used of double-quotes to define a substitution expressions&lt;/p&gt;
&lt;h2 id=&#34;builtin-functions&#34;&gt;Built-in Functions&lt;/h2&gt;
&lt;p&gt;ScEL supports a number of predefined functions that can be used to apply a single transformation on a field.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Syntax&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;contains&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an array field&#39;s value contains the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ contains(array, &#39;value&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;converts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts a field&#39;value into the specified type&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ converts(field, INTEGER) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ends_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value end with the specified string suffix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ ends_with(field, &#39;suffix&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;equals&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string or number fields&#39;s value equals the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ equals(field, value) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exists&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an the specified field exists&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ ends_with(field, value) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;extract_array&lt;/code&gt;| Returns the element at the specified position of the specified array&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{extract_array(array, 0) }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;is_null&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ is_null(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;length&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns the number of elements into an array of the length of an string field&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ length(array) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;lowercase&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts all of the characters in a string field&#39;s value to lower case&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ lowercase(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;matches&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value match the specified regex&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ matches(field, &#39;regex&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nlv&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Sets a default value if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ length(array) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;replace_all &lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Replaces every subsequence of the field&#39;s value that matches the given pattern with the given replacement string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ replace_all(field, &#39;regex&#39;, &#39;replacement&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;starts_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value start with the specified string prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ starts_with(field, &#39;prefix&#39;) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;trim&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Trims the spaces from the beginning and end of a string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ trim(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;uppercase&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts all of the characters in a string field&#39;s value to upper case&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ uppercase(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In addition, ScEL supports nested functions.&lt;/p&gt;
&lt;p&gt;For example, the following expression is used to replace all whitespace characters after transforming our field&#39;s value into lowercase.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;replace_all(lowercase($.field), &#39;\\s&#39;, &#39;-&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Limitation&lt;/h4&gt;
Currently, FilePulse does not support user-defined functions (UDFs). So you cannot register your own functions to enrich the expression language.
&lt;/div&gt;

&lt;h2 id=&#34;scopes&#34;&gt;Scopes&lt;/h2&gt;
&lt;p&gt;In previous section, we have shown how to use the expression language to select a specific field.
The selected field was part of our the current record being processed.&lt;/p&gt;
&lt;p&gt;Actually, ScEL allows you to get access to additional fields through the used of scopes.
Basically, a scope defined the root object on which a selector expression must evaluated.&lt;/p&gt;
&lt;p&gt;The syntax to define an expression with a scope is of the form : &amp;ldquo;&lt;code&gt;$&amp;lt;scope&amp;gt;.&amp;lt;selector expression string&amp;gt;&lt;/code&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;By default, if no scope is defined in the expression, the scope &lt;code&gt;$value&lt;/code&gt; is implicitly used.&lt;/p&gt;
&lt;p&gt;ScEL supports a number of predefined scopes that can be used for example :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;To define the topic for the record.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To define the key for the record.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To get access to metadata about the source file.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Scope&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$headers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record headers&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$key&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record key&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file metadata&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The offset information of this record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$system&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment variables and runtime properties&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$timestamp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record timestamp&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$topic&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The output topic&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$value&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$variables&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The contextual filter-chain variables&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, object]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note, that in case of failures more fields are added to the current filter context (see : &lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/handling-failures/&#34;&gt;Handling Failures&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&#34;record-headers&#34;&gt;Record Headers&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;headers&lt;/code&gt; allows to defined the headers of the output record.&lt;/p&gt;
&lt;h3 id=&#34;record-key&#34;&gt;Record key&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;key&lt;/code&gt; allows to defined the key of the output record. Only string key is currently supported.&lt;/p&gt;
&lt;h3 id=&#34;source-metadata&#34;&gt;Source Metadata&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;metadata&lt;/code&gt; allows read access to information about the file being processing.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file directory path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.absolutePath&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file absolute path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.hash&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file CRC32 hash&lt;/td&gt;
&lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.lastModified&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file last modified time.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file size&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$metadata.inode&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file Unix inode&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;record-offset&#34;&gt;Record Offset&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;offset&lt;/code&gt; allows read access to information about the original position of the record into the source file.
The available fields depend of the configured FileInputRecord.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.timestamp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The creation time of the record (millisecond)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;RowFilterReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.startPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.endPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The end position of the record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The size in bytes&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.row&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The row number of the record into the source&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;BytesArrayInputReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.startPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the record into the source file (always equals to 0)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.endPosition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The end position of the record into the source file (equals to the file size)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;AvroFilterInputReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.blockStart&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the current block&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.position&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The position into the current block.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$offset.records&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The number of record read into the current block.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;system&#34;&gt;System&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;system&lt;/code&gt; allows read access to system environment variables and runtime properties.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$system.env&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment variables.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, string]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;$system.props&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment properties.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, string]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;timestamp&#34;&gt;Timestamp&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$timestamp&lt;/code&gt; allows to defined the timestamp of the output record.&lt;/p&gt;
&lt;h2 id=&#34;topic&#34;&gt;Topic&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$topic&lt;/code&gt; allows to defined the target topic of the output record.&lt;/p&gt;
&lt;h2 id=&#34;value&#34;&gt;Value&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$value&lt;/code&gt; allows to defined the fields of the output record&lt;/p&gt;
&lt;h2 id=&#34;variables&#34;&gt;Variables&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;$variables&lt;/code&gt; allows read/write access to a simple key-value map structure.
This scope can be used to share user-defined variables between &lt;a href=&#34;https://streamthoughts.github.io/kafka-connect-file-pulse/kafka-connect-file-pulse/docs/developer-guide/filters/&#34;&gt;Processing Filters&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note : variables are not cached between records.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Accessing Data and Metadata</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/developer-guide/accessing-data-and-metadata/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/developer-guide/accessing-data-and-metadata/</guid>
      <description>
        
        
        &lt;p&gt;Some filters (e.g : &lt;a href=&#34;#appendfilter&#34;&gt;AppendFilter&lt;/a&gt;) can be configured using &lt;em&gt;Simple Connect Expression Language&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Simple Connect Expression Language&lt;/em&gt; (ScEL for short) is an expression language based on regex that allows quick access and manipulating record fields and metadata.&lt;/p&gt;
&lt;p&gt;The syntax to define an expression is of the form : &amp;ldquo;&lt;code&gt;{{ &amp;lt;expression string&amp;gt; }}&lt;/code&gt;&amp;rdquo;.&lt;/p&gt;


&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Note&lt;/h4&gt;
In some situation double brackets can be omitted if the expression is used to write a value into a target field.
&lt;/div&gt;

&lt;p&gt;ScEL supports the following capabilities :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Field Selector&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nested Navigation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;String substitution&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functions&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;field-selector&#34;&gt;Field Selector&lt;/h2&gt;
&lt;p&gt;The expression language can be used to easily select one field from the input record :&lt;/p&gt;
&lt;p&gt;&amp;ldquo;&lt;code&gt;{{ username }}&lt;/code&gt;&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;nested-navigation&#34;&gt;Nested Navigation&lt;/h2&gt;
&lt;p&gt;To navigate down a struct value, just use a period to indicate a nested field value :&lt;/p&gt;
&lt;p&gt;&amp;ldquo;&lt;code&gt;{{ address.city }}&lt;/code&gt;&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;string-substitution&#34;&gt;String substitution&lt;/h2&gt;
&lt;p&gt;The expression language can be used to easily build a new string field that concatenate multiple ones :&lt;/p&gt;
&lt;p&gt;&amp;ldquo;&lt;code&gt;{{ &amp;lt;expression one&amp;gt; }}-{{ &amp;lt;expression two&amp;gt;}}&lt;/code&gt;&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;builtin-functions&#34;&gt;Built-in Functions&lt;/h2&gt;
&lt;p&gt;ScEL supports a number of predefined functions that can be used to apply a single transformation on a field.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Syntax&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;contains&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an array field&#39;s value contains the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ contains(array, value) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;converts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts a field&#39;value into the specified type&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ converts(field, INTEGER) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ends_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value end with the specified string suffix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ ends_with(field, suffix) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;equals&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string or number fields&#39;s value equals the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ equals(field, value) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exists&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an the specified field exists&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ ends_with(field, value) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;extract_array&lt;/code&gt;| Returns the element at the specified position of the specified array&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{extract_array(array, 0) }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;is_null&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ is_null(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;length&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns the number of elements into an array of the length of an string field&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ length(array) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;lowercase&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts all of the characters in a string field&#39;s value to lower case&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ lowercase(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;matches&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value match the specified regex&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ matches(field, regex) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nlv&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Sets a default value if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ length(array) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;replace_all &lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Replaces every subsequence of the field&#39;s value that matches the given pattern with the given replacement string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ replace_all(field, regex, replacement) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;starts_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value start with the specified string prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ starts_with(field, prefix) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;trim&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Trims the spaces from the beginning and end of a string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ trim(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;uppercase&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Converts all of the characters in a string field&#39;s value to upper case&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ uppercase(field) }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In addition, ScEL supports nested functions.&lt;/p&gt;
&lt;p&gt;For example, the following expression is used to replace all whitespace characters after transforming our field&#39;s value into lowercase.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{ replace_all(lowercase(field), \\s, -)}}
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Limitation&lt;/h4&gt;
Currently, FilePulse does not support user-defined functions (UDFs). So you cannot register your own functions to enrich the expression language.
&lt;/div&gt;

&lt;h2 id=&#34;scopes&#34;&gt;Scopes&lt;/h2&gt;
&lt;p&gt;In previous section, we have shown how to use the expression language to select a specific field.
The selected field was part of our the current record being processed.&lt;/p&gt;
&lt;p&gt;Actually, ScEL allows you to get access to additional fields through the used of scopes.
Basically, a scope defined the root object on which a selector expression must evaluated.&lt;/p&gt;
&lt;p&gt;The syntax to define an expression with a scope is of the form : &amp;ldquo;&lt;code&gt;{{ $&amp;lt;scope&amp;gt;.&amp;lt;selector expression string&amp;gt; }}&lt;/code&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;By default, if no scope is defined in the expression, the scope &lt;code&gt;$value&lt;/code&gt; is implicitly used.&lt;/p&gt;
&lt;p&gt;ScEL supports a number of predefined scopes that can be used for example :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;To override the output topic.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To define record the key to be used.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To get access to the source file metadata.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Scope&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $headers }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record headers&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $key }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record key&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $metadata }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file metadata&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $offset }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The offset information of this record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $system }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment variables and runtime properties&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $timestamp }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record timestamp&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $topic }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The output topic&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $value }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The record value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;struct&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $variables }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The contextual filter-chain variables&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, object]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note, that in case of failures more fields are added to the current filter context (see : &lt;a href=&#34;./handling-failures&#34;&gt;Handling Failures&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;record-headers&#34;&gt;Record Headers&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;headers&lt;/code&gt; allows to defined the headers of the output record.&lt;/p&gt;
&lt;h3 id=&#34;record-key&#34;&gt;Record key&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;key&lt;/code&gt; allows to defined the key of the output record. Only string key is currently supported.&lt;/p&gt;
&lt;h3 id=&#34;source-metadata&#34;&gt;Source Metadata&lt;/h3&gt;
&lt;p&gt;The scope &lt;code&gt;metadata&lt;/code&gt; allows read access to information about the file being processing.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $metadata.name }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $metadata.path }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file directory path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $metadata.absolutePath }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file absolute path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $metadata.hash }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file CRC32 hash&lt;/td&gt;
&lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $metadata.lastModified }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file last modified time.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $metadata.size }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file size&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $metadata.inode }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The file Unix inode&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;record-offset&#34;&gt;Record Offset&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;offset&lt;/code&gt; allows read access to information about the original position of the record into the source file.
The available fields depend of the configured FileInputRecord.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $offset.timestamp }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The creation time of the record (millisecond)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;RowFilterReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $offset.startPosition }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $offset.endPosition }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The end position of the record into the source file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $offset.size }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The size in bytes&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $offset.row }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The row number of the record into the source&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;BytesArrayInputReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $offset.startPosition }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the record into the source file (always equals to 0)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $offset.endPosition }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The end position of the record into the source file (equals to the file size)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Information only available if &lt;code&gt;AvroFilterInputReader&lt;/code&gt; is configured.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $offset.blockStart }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The start position of the current block&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $offset.position }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The position into the current block.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $offset.records }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The number of record read into the current block.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;long&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;system&#34;&gt;System&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;system&lt;/code&gt; allows read access to system environment variables and runtime properties.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predefined Fields (ScEL)&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $system.env }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment variables.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, string]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;{{ $system.props }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The system environment properties.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;map[string, string]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;timestamp&#34;&gt;Timestamp&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;timestamp&lt;/code&gt; allows to defined the timestamp of the output record.&lt;/p&gt;
&lt;h2 id=&#34;topic&#34;&gt;Topic&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;topic&lt;/code&gt; allows to defined the target topic of the output record.&lt;/p&gt;
&lt;h2 id=&#34;value&#34;&gt;Value&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;value&lt;/code&gt; allows to defined the fields of the output record&lt;/p&gt;
&lt;h2 id=&#34;variables&#34;&gt;Variables&lt;/h2&gt;
&lt;p&gt;The scope &lt;code&gt;variables&lt;/code&gt; allows read/write access to a simple key-value map structure.
This scope can be used to share user-defined variables between filters.&lt;/p&gt;
&lt;p&gt;Note : variables are not cached between records.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Conditional Execution</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/developer-guide/conditional-execution/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.3.x/developer-guide/conditional-execution/</guid>
      <description>
        
        
        &lt;p&gt;A conditional property &lt;code&gt;if&lt;/code&gt; can be configured on each filter to determine if that filter should be applied or skipped.
When a filter is skipped, message flow to the next filter without any modification.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;if&lt;/code&gt; configuration accepts a Simple Connect Expression that must return to &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt;.
If the configured expression does not evaluate to a boolean value the filter chain will failed.&lt;/p&gt;
&lt;p&gt;The&lt;code&gt;if&lt;/code&gt; property supports (&lt;a href=&#34;accessing-data-and-metadata&#34;&gt;simple expression&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The boolean value returned from the filter condition can be inverted by setting the property &lt;code&gt;invert&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, the below filter will only be applied on message having a log message containing &amp;ldquo;BadCredentialsException&amp;rdquo;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filters.TagSecurityException.type=io.streamthoughts.kafka.connect.filepulse.filter.AppendFilter
filters.TagSecurityException.if={{ contains(data.logmessage, BadCredentialsException) }}
filters.TagSecurityException.invert=false
filters.TagSecurityException.field=tags
filters.TagSecurityException.values=SecurityAlert
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These boolean functions are available for use with &lt;code&gt;if&lt;/code&gt; configuration :&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Syntax&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;contains&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an array field&#39;s value contains the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ contains(field, value) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ends_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value end with the specified string suffix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ ends_with(field, suffix) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;equals&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string or number fields&#39;s value equals the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ equals(field, value) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exists&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an the specified field exists&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ exists(struct, field) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;is_null&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ is_null(field) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;matches&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value match the specified regex&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ matches(field, regex) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;starts_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value start with the specified string prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ starts_with(field, prefix) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;if&lt;/code&gt; property does not support binary operator and then a single condition can be configured.&lt;/li&gt;
&lt;li&gt;condition cannot be used to easily create pipeline branching.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Conditional Execution</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/developer-guide/conditional-execution/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.4.x/developer-guide/conditional-execution/</guid>
      <description>
        
        
        &lt;p&gt;A conditional property &lt;code&gt;if&lt;/code&gt; can be configured on each filter to determine if that filter should be applied or skipped.
When a filter is skipped, message flow to the next filter without any modification.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;if&lt;/code&gt; configuration accepts a Simple Connect Expression that must return to &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt;.
If the configured expression does not evaluate to a boolean value the filter chain will failed.&lt;/p&gt;
&lt;p&gt;The&lt;code&gt;if&lt;/code&gt; property supports (&lt;a href=&#34;accessing-data-and-metadata&#34;&gt;simple expression&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The boolean value returned from the filter condition can be inverted by setting the property &lt;code&gt;invert&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, the below filter will only be applied on message having a log message containing &amp;ldquo;BadCredentialsException&amp;rdquo;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filters.TagSecurityException.type=io.streamthoughts.kafka.connect.filepulse.filter.AppendFilter
filters.TagSecurityException.if={{ contains(data.logmessage, BadCredentialsException) }}
filters.TagSecurityException.invert=false
filters.TagSecurityException.field=tags
filters.TagSecurityException.values=SecurityAlert
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These boolean functions are available for use with &lt;code&gt;if&lt;/code&gt; configuration :&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Syntax&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;contains&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an array field&#39;s value contains the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ contains(field, value) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ends_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value end with the specified string suffix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ ends_with(field, suffix) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;equals&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string or number fields&#39;s value equals the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ equals(field, value) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exists&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an the specified field exists&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ exists(struct, field) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;is_null&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ is_null(field) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;matches&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value match the specified regex&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ matches(field, regex) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;starts_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value start with the specified string prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ starts_with(field, prefix) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;if&lt;/code&gt; property does not support binary operator and then a single condition can be configured.&lt;/li&gt;
&lt;li&gt;condition cannot be used to easily create pipeline branching.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Conditional Execution</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/developer-guide/conditional-execution/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.5.x/developer-guide/conditional-execution/</guid>
      <description>
        
        
        &lt;p&gt;A conditional property &lt;code&gt;if&lt;/code&gt; can be configured on each filter to determine if that filter should be applied or skipped.
When a filter is skipped, message flow to the next filter without any modification.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;if&lt;/code&gt; configuration accepts a Simple Connect Expression that must return to &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt;.
If the configured expression does not evaluate to a boolean value the filter chain will failed.&lt;/p&gt;
&lt;p&gt;The&lt;code&gt;if&lt;/code&gt; property supports (&lt;a href=&#34;accessing-data-and-metadata&#34;&gt;simple expression&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The boolean value returned from the filter condition can be inverted by setting the property &lt;code&gt;invert&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, the below filter will only be applied on message having a log message containing &amp;ldquo;BadCredentialsException&amp;rdquo;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filters.TagSecurityException.type=io.streamthoughts.kafka.connect.filepulse.filter.AppendFilter
filters.TagSecurityException.if={{ contains(data.logmessage, BadCredentialsException) }}
filters.TagSecurityException.invert=false
filters.TagSecurityException.field=tags
filters.TagSecurityException.values=SecurityAlert
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These boolean functions are available for use with &lt;code&gt;if&lt;/code&gt; configuration :&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Syntax&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;contains&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an array field&#39;s value contains the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ contains(field, value) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ends_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value end with the specified string suffix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ ends_with(field, suffix) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;equals&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string or number fields&#39;s value equals the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ equals(field, value) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exists&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an the specified field exists&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ exists(struct, field) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;is_null&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ is_null(field) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;matches&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value match the specified regex&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ matches(field, regex) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;starts_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value start with the specified string prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ starts_with(field, prefix) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;if&lt;/code&gt; property does not support binary operator and then a single condition can be configured.&lt;/li&gt;
&lt;li&gt;condition cannot be used to easily create pipeline branching.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Conditional Execution</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/developer-guide/conditional-execution/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v1.6.x/developer-guide/conditional-execution/</guid>
      <description>
        
        
        &lt;p&gt;A conditional property &lt;code&gt;if&lt;/code&gt; can be configured on each filter to determine if that filter should be applied or skipped.
When a filter is skipped, message flow to the next filter without any modification.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;if&lt;/code&gt; configuration accepts a Simple Connect Expression that must return to &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt;.
If the configured expression does not evaluate to a boolean value the filter chain will failed.&lt;/p&gt;
&lt;p&gt;The&lt;code&gt;if&lt;/code&gt; property supports (&lt;a href=&#34;accessing-data-and-metadata&#34;&gt;simple expression&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The boolean value returned from the filter condition can be inverted by setting the property &lt;code&gt;invert&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, the below filter will only be applied on message having a log message containing &amp;ldquo;BadCredentialsException&amp;rdquo;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filters.TagSecurityException.type=io.streamthoughts.kafka.connect.filepulse.filter.AppendFilter
filters.TagSecurityException.if={{ contains(data.logmessage, BadCredentialsException) }}
filters.TagSecurityException.invert=false
filters.TagSecurityException.field=tags
filters.TagSecurityException.values=SecurityAlert
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These boolean functions are available for use with &lt;code&gt;if&lt;/code&gt; configuration :&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Syntax&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;contains&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an array field&#39;s value contains the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ contains(field, value) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ends_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value end with the specified string suffix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ ends_with(field, suffix) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;equals&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string or number fields&#39;s value equals the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ equals(field, value) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exists&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an the specified field exists&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ exists(struct, field) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;is_null&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ is_null(field) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;matches&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value match the specified regex&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ matches(field, regex) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;starts_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value start with the specified string prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ starts_with(field, prefix) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;if&lt;/code&gt; property does not support binary operator and then a single condition can be configured.&lt;/li&gt;
&lt;li&gt;condition cannot be used to easily create pipeline branching.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Conditional Execution</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/conditional-execution/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/archives/v2.0.x/developer-guide/conditional-execution/</guid>
      <description>
        
        
        &lt;p&gt;A conditional property &lt;code&gt;if&lt;/code&gt; can be configured on each filter to determine if that filter should be applied or skipped.
When a filter is skipped, message flow to the next filter without any modification.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;if&lt;/code&gt; configuration accepts a Simple Connect Expression that must return to &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt;.
If the configured expression does not evaluate to a boolean value the filter chain will failed.&lt;/p&gt;
&lt;p&gt;The&lt;code&gt;if&lt;/code&gt; property supports (&lt;a href=&#34;accessing-data-and-metadata&#34;&gt;simple expression&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The boolean value returned from the filter condition can be inverted by setting the property &lt;code&gt;invert&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, the below filter will only be applied on message having a log message containing &amp;ldquo;BadCredentialsException&amp;rdquo;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filters.TagSecurityException.type=io.streamthoughts.kafka.connect.filepulse.filter.AppendFilter
filters.TagSecurityException.if={{ contains(data.logmessage, BadCredentialsException) }}
filters.TagSecurityException.invert=false
filters.TagSecurityException.field=tags
filters.TagSecurityException.values=SecurityAlert
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These boolean functions are available for use with &lt;code&gt;if&lt;/code&gt; configuration :&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Syntax&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;contains&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an array field&#39;s value contains the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ contains(field, value) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ends_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value end with the specified string suffix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ ends_with(field, suffix) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;equals&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string or number fields&#39;s value equals the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ equals(field, value) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exists&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an the specified field exists&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ exists(struct, field) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;is_null&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ is_null(field) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;matches&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value match the specified regex&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ matches(field, regex) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;starts_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value start with the specified string prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ starts_with(field, prefix) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;if&lt;/code&gt; property does not support binary operator and then a single condition can be configured.&lt;/li&gt;
&lt;li&gt;condition cannot be used to easily create pipeline branching.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Conditional Execution</title>
      <link>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/conditional-execution/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://streamthoughts.github.io/kafka-connect-file-pulse/docs/developer-guide/conditional-execution/</guid>
      <description>
        
        
        &lt;p&gt;A conditional property &lt;code&gt;if&lt;/code&gt; can be configured on each filter to determine if that filter should be applied or skipped.
When a filter is skipped, message flow to the next filter without any modification.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;if&lt;/code&gt; configuration accepts a Simple Connect Expression that must return to &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt;.
If the configured expression does not evaluate to a boolean value the filter chain will failed.&lt;/p&gt;
&lt;p&gt;The&lt;code&gt;if&lt;/code&gt; property supports (&lt;a href=&#34;accessing-data-and-metadata&#34;&gt;simple expression&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The boolean value returned from the filter condition can be inverted by setting the property &lt;code&gt;invert&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, the below filter will only be applied on message having a log message containing &amp;ldquo;BadCredentialsException&amp;rdquo;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filters.TagSecurityException.type=io.streamthoughts.kafka.connect.filepulse.filter.AppendFilter
filters.TagSecurityException.if={{ contains(data.logmessage, BadCredentialsException) }}
filters.TagSecurityException.invert=false
filters.TagSecurityException.field=tags
filters.TagSecurityException.values=SecurityAlert
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These boolean functions are available for use with &lt;code&gt;if&lt;/code&gt; configuration :&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Syntax&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;contains&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an array field&#39;s value contains the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ contains(field, value) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ends_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value end with the specified string suffix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ ends_with(field, suffix) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;equals&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string or number fields&#39;s value equals the specified value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ equals(field, value) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exists&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an the specified field exists&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ exists(struct, field) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;is_null&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value is null&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ is_null(field) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;matches&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if a field&#39;s value match the specified regex&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ matches(field, regex) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;starts_with&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Returns &lt;code&gt;true&lt;/code&gt; if an a string field&#39;s value start with the specified string prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{% raw %}{{ starts_with(field, prefix) }}{% endraw %}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;if&lt;/code&gt; property does not support binary operator and then a single condition can be configured.&lt;/li&gt;
&lt;li&gt;condition cannot be used to easily create pipeline branching.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
